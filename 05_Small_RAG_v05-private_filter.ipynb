{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9bf820",
   "metadata": {},
   "source": [
    "# GreenPower RAG System v05 - Private Data Filter\n",
    "\n",
    "## Pipeline complet:\n",
    "1. Setup Qdrant (local vectoriel)\n",
    "2. Chargement documents GreenPower via Gradio\n",
    "3. Chunking + Embeddings (sentence-transformers)\n",
    "4. Stockage Qdrant vector index avec **mÃ©tadonnÃ©es de confidentialitÃ©**\n",
    "5. RAG pipeline : query â†’ retrieval â†’ **filtrage private_** â†’ Mistral\n",
    "6. Interface Gradio interactive\n",
    "\n",
    "## NouveautÃ©s v05:\n",
    "- âœ¨ **SystÃ¨me de confidentialitÃ©**: documents avec prÃ©fixe `private_` sont filtrÃ©s\n",
    "- âœ¨ **DÃ©tection automatique**: lors de l'upload, dÃ©tecte les fichiers privÃ©s\n",
    "- âœ¨ **Message de confidentialitÃ©**: \"DÃ©solÃ©, donnÃ©e confidentielle\" pour les rÃ©sultats privÃ©s\n",
    "- âœ¨ **Logging transparent**: affiche quand des rÃ©sultats confidentiels sont exclus\n",
    "\n",
    "## Comment marquer un document comme privÃ©:\n",
    "- Nommez votre fichier avec le prÃ©fixe `private_` (ex: `private_salaires.pdf`)\n",
    "- Le systÃ¨me dÃ©tectera automatiquement et ajoutera un flag `is_private: true`\n",
    "- Ces chunks ne seront jamais affichÃ©s dans les rÃ©ponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Installation des dÃ©pendances\n",
    "!pip install -q langchain-mistralai langchain-community langchain-text-splitters\n",
    "!pip install -q qdrant-client gradio sentence-transformers\n",
    "!pip install -q pypdf python-docx python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "from typing import List\n",
    "import pypdf\n",
    "import docx\n",
    "import json\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = Path('.env')\n",
    "if env_path.exists():\n",
    "    with open(env_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#') and '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "QDRANT_URL = os.getenv('QDRANT_URL', ':memory:')  # Use :memory: for local or cloud URL\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_clients",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialisation des clients\n",
    "\n",
    "# Configuration CHUNKS\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Initialize components\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "llm = ChatMistralAI(model='mistral-small-latest', mistral_api_key=MISTRAL_API_KEY, temperature=0.7)\n",
    "\n",
    "qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "COLLECTION_NAME = \"greenpower_docs\"\n",
    "\n",
    "# Text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "print(\"âœ… Clients initialisÃ©s\")\n",
    "print(f\"   - Qdrant: Mode {QDRANT_URL}\")\n",
    "print(f\"   - Embeddings: {EMBEDDING_MODEL}\")\n",
    "print(f\"   - LLM: Mistral Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: CrÃ©ation de la collection Qdrant\n",
    "\n",
    "# Obtenir la dimension des embeddings en gÃ©nÃ©rant un embedding de test\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "VECTOR_SIZE = len(test_embedding)\n",
    "\n",
    "def create_collection_if_not_exists():\n",
    "    \"\"\"CrÃ©e la collection Qdrant si elle n'existe pas dÃ©jÃ \"\"\"\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections()\n",
    "        collection_names = [c.name for c in collections.collections]\n",
    "        \n",
    "        if COLLECTION_NAME in collection_names:\n",
    "            print(f\"â„¹ï¸ Collection '{COLLECTION_NAME}' existe dÃ©jÃ \")\n",
    "            return\n",
    "        \n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(\n",
    "                size=VECTOR_SIZE,\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        print(f\"âœ… Collection '{COLLECTION_NAME}' crÃ©Ã©e (dimension: {VECTOR_SIZE})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur crÃ©ation collection: {e}\")\n",
    "        raise\n",
    "\n",
    "# CrÃ©er la collection\n",
    "create_collection_if_not_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Fonctions utilitaires\n",
    "\n",
    "def extract_text_from_file(file_path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF, DOCX, TXT, JSON ou CSV\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    try:\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            with open(file_path, 'rb') as f:\n",
    "                pdf_reader = pypdf.PdfReader(f)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        \n",
    "        elif file_path.suffix.lower() in ['.docx', '.doc']:\n",
    "            doc = docx.Document(file_path)\n",
    "            return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.json':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                # Convertir le JSON en texte lisible\n",
    "                if isinstance(data, dict):\n",
    "                    text_parts = []\n",
    "                    for key, value in data.items():\n",
    "                        if isinstance(value, (dict, list)):\n",
    "                            text_parts.append(f\"{key}: {json.dumps(value, ensure_ascii=False, indent=2)}\")\n",
    "                        else:\n",
    "                            text_parts.append(f\"{key}: {value}\")\n",
    "                    return \"\\n\".join(text_parts)\n",
    "                elif isinstance(data, list):\n",
    "                    return \"\\n\\n\".join([json.dumps(item, ensure_ascii=False, indent=2) for item in data])\n",
    "                else:\n",
    "                    return str(data)\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.csv':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                csv_reader = csv.DictReader(f)\n",
    "                rows = []\n",
    "                for row in csv_reader:\n",
    "                    row_text = \", \".join([f\"{key}: {value}\" for key, value in row.items()])\n",
    "                    rows.append(row_text)\n",
    "                return \"\\n\".join(rows)\n",
    "        \n",
    "        else:\n",
    "            return f\"Format non supportÃ©: {file_path.suffix}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Erreur lecture fichier {file_path.name}: {str(e)}\"\n",
    "\n",
    "def is_private_file(filename: str) -> bool:\n",
    "    \"\"\"DÃ©termine si un fichier est privÃ© basÃ© sur son nom\"\"\"\n",
    "    return filename.lower().startswith('private_')\n",
    "\n",
    "def chunk_and_embed(text: str, source: str, is_private: bool = False) -> List[dict]:\n",
    "    \"\"\"DÃ©coupe le texte en chunks et gÃ©nÃ¨re les embeddings avec flag de confidentialitÃ©\"\"\"\n",
    "    # Chunking\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Embeddings\n",
    "    chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "    \n",
    "    # PrÃ©parer les points pour Qdrant\n",
    "    points = []\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        point = PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector=embedding,\n",
    "            payload={\n",
    "                \"text\": chunk,\n",
    "                \"source\": source,\n",
    "                \"chunk_index\": i,\n",
    "                \"is_private\": is_private  # ğŸ”’ FLAG DE CONFIDENTIALITÃ‰\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    return points\n",
    "\n",
    "print(\"âœ… Fonctions utilitaires dÃ©finies (avec dÃ©tection private_)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Fonction d'upload de documents avec dÃ©tection private_\n",
    "\n",
    "def upload_documents(files):\n",
    "    \"\"\"Traite et stocke les documents uploadÃ©s avec dÃ©tection de confidentialitÃ©\"\"\"\n",
    "    if not files:\n",
    "        return \"âŒ Aucun fichier uploadÃ©\"\n",
    "    \n",
    "    # S'assurer que la collection existe\n",
    "    create_collection_if_not_exists()\n",
    "    \n",
    "    total_chunks = 0\n",
    "    private_chunks = 0\n",
    "    results = []\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            # Extraction du texte\n",
    "            file_path = file.name if hasattr(file, 'name') else file\n",
    "            filename = Path(file_path).name\n",
    "            \n",
    "            # ğŸ”’ DÃ©tection si le fichier est privÃ©\n",
    "            is_private = is_private_file(filename)\n",
    "            \n",
    "            text = extract_text_from_file(file_path)\n",
    "            \n",
    "            if text.startswith(\"Erreur\") or text.startswith(\"Format non supportÃ©\"):\n",
    "                results.append(f\"âš ï¸ {filename}: {text}\")\n",
    "                continue\n",
    "            \n",
    "            # Chunking et embedding avec flag private\n",
    "            points = chunk_and_embed(text, filename, is_private=is_private)\n",
    "            \n",
    "            # Stockage dans Qdrant\n",
    "            qdrant_client.upsert(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                points=points\n",
    "            )\n",
    "            \n",
    "            total_chunks += len(points)\n",
    "            if is_private:\n",
    "                private_chunks += len(points)\n",
    "                results.append(f\"ğŸ”’ {filename}: {len(points)} chunks (PRIVÃ‰ - ne sera pas affichÃ©)\")\n",
    "            else:\n",
    "                results.append(f\"âœ… {filename}: {len(points)} chunks (PUBLIC)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append(f\"âŒ {filename}: Erreur - {str(e)}\")\n",
    "    \n",
    "    summary = f\"\\n\\nğŸ“Š **Total: {total_chunks} chunks** stockÃ©s dans '{COLLECTION_NAME}'\\n\"\n",
    "    summary += f\"   - ğŸ”“ Public: {total_chunks - private_chunks} chunks\\n\"\n",
    "    summary += f\"   - ğŸ”’ PrivÃ©: {private_chunks} chunks\"\n",
    "    \n",
    "    return \"\\n\".join(results) + summary\n",
    "\n",
    "print(\"âœ… Fonction upload_documents dÃ©finie (avec marquage private_)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Fonction RAG avec filtrage des donnÃ©es privÃ©es\n",
    "\n",
    "def search_and_answer(question: str, top_k: int = 3):\n",
    "    \"\"\"Recherche dans Qdrant et gÃ©nÃ¨re une rÃ©ponse avec Mistral (filtre les donnÃ©es privÃ©es)\"\"\"\n",
    "    if not question or question.strip() == \"\":\n",
    "        return \"âš ï¸ Veuillez poser une question\"\n",
    "    \n",
    "    try:\n",
    "        # 1. GÃ©nÃ©rer l'embedding de la question\n",
    "        question_embedding = embeddings.embed_query(question)\n",
    "        \n",
    "        # 2. Recherche dans Qdrant avec un top_k plus large pour compenser le filtrage\n",
    "        search_results = qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=question_embedding,\n",
    "            limit=top_k * 3  # Chercher 3x plus pour avoir assez aprÃ¨s filtrage\n",
    "        )\n",
    "        \n",
    "        # 3. ğŸ”’ FILTRAGE DES DONNÃ‰ES PRIVÃ‰ES\n",
    "        filtered_results = []\n",
    "        private_count = 0\n",
    "        \n",
    "        for point in search_results:\n",
    "            # VÃ©rifier si le chunk est privÃ©\n",
    "            if point.payload.get(\"is_private\", False):\n",
    "                private_count += 1\n",
    "                continue  # ğŸš« Ignorer les chunks privÃ©s\n",
    "            \n",
    "            filtered_results.append(point)\n",
    "            \n",
    "            # Limiter au top_k demandÃ©\n",
    "            if len(filtered_results) >= top_k:\n",
    "                break\n",
    "        \n",
    "        # 4. VÃ©rifier s'il reste des rÃ©sultats aprÃ¨s filtrage\n",
    "        if not filtered_results:\n",
    "            if private_count > 0:\n",
    "                return \"\"\"## ğŸ”’ DonnÃ©e confidentielle\n",
    "\n",
    "DÃ©solÃ©, les informations pertinentes pour cette question sont marquÃ©es comme **confidentielles** et ne peuvent pas Ãªtre affichÃ©es.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ”’ **{} rÃ©sultats privÃ©s** ont Ã©tÃ© trouvÃ©s mais masquÃ©s pour des raisons de confidentialitÃ©.\n",
    "\"\"\".format(private_count)\n",
    "            else:\n",
    "                return \"âš ï¸ Aucun document trouvÃ©. Uploadez d'abord des documents.\"\n",
    "        \n",
    "        # 5. Extraire les chunks pertinents (uniquement publics)\n",
    "        contexts = []\n",
    "        sources = []\n",
    "        for point in filtered_results:\n",
    "            contexts.append(point.payload[\"text\"])\n",
    "            source = point.payload.get(\"source\", \"Unknown\")\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        \n",
    "        context_text = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "        \n",
    "        # 6. Construire le prompt\n",
    "        prompt = f\"\"\"Tu es un assistant qui rÃ©pond aux questions en te basant UNIQUEMENT sur le contexte fourni.\n",
    "\n",
    "Contexte:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- RÃ©ponds de maniÃ¨re claire et concise\n",
    "- Base-toi UNIQUEMENT sur le contexte fourni\n",
    "- Si l'information n'est pas dans le contexte, dis-le clairement\n",
    "- Cite les sources quand pertinent\n",
    "\n",
    "RÃ©ponse:\"\"\"\n",
    "        \n",
    "        # 7. GÃ©nÃ©rer la rÃ©ponse avec Mistral\n",
    "        response = llm.invoke(prompt)\n",
    "        answer = response.content\n",
    "        \n",
    "        # 8. Formater la rÃ©ponse avec info sur filtrage\n",
    "        output = f\"\"\"## ğŸ’¬ RÃ©ponse\n",
    "\n",
    "{answer}\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ“š **Sources consultÃ©es:** {', '.join(sources)}\n",
    "ğŸ” **{len(contexts)} chunks** analysÃ©s (publics uniquement)\n",
    "\"\"\"\n",
    "        \n",
    "        if private_count > 0:\n",
    "            output += f\"\\nğŸ”’ **{private_count} rÃ©sultats privÃ©s** ont Ã©tÃ© exclus de cette recherche\"\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"âŒ Erreur: {str(e)}\\n\\nDÃ©tails technique: {type(e).__name__}\"\n",
    "\n",
    "print(\"âœ… Fonction search_and_answer dÃ©finie (avec filtrage private_)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio_interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Interface Gradio\n",
    "\n",
    "with gr.Blocks(title=\"GreenPower RAG v05 - Private Filter\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ğŸŒ± GreenPower RAG System v05 - ğŸ”’ Private Data Filter\n",
    "        \n",
    "        **Retrieval-Augmented Generation** pour vos documents GreenPower avec **gestion de confidentialitÃ©**\n",
    "        \n",
    "        ### ğŸ“‹ Workflow:\n",
    "        1. **Upload** vos documents (PDF, DOCX, TXT, JSON, CSV)\n",
    "        2. Les fichiers avec prÃ©fixe `private_` seront marquÃ©s comme **confidentiels**\n",
    "        3. **Ask** vos questions sur le contenu\n",
    "        4. Les donnÃ©es privÃ©es ne seront **jamais affichÃ©es** dans les rÃ©ponses\n",
    "        \n",
    "        ### ğŸ”’ ConfidentialitÃ©:\n",
    "        - Fichiers commenÃ§ant par `private_` â†’ ğŸ”’ **MasquÃ©s automatiquement**\n",
    "        - Ex: `private_salaires.pdf`, `private_contrats.docx`\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Tab(\"ğŸ“¤ Upload Documents\"):\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### Upload vos documents\n",
    "            \n",
    "            Formats supportÃ©s: **PDF**, **DOCX**, **TXT**, **JSON**, **CSV**\n",
    "            \n",
    "            ğŸ”’ **Documents privÃ©s:** Nommez vos fichiers avec le prÃ©fixe `private_`\n",
    "            - `private_budget.pdf` â†’ Sera marquÃ© comme confidentiel\n",
    "            - `rapport_public.pdf` â†’ Sera accessible dans les rÃ©ponses\n",
    "            \n",
    "            Les documents seront:\n",
    "            - DÃ©coupÃ©s en chunks\n",
    "            - VectorisÃ©s avec sentence-transformers\n",
    "            - StockÃ©s dans Qdrant avec mÃ©tadonnÃ©es de confidentialitÃ©\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        file_input = gr.File(\n",
    "            file_count=\"multiple\",\n",
    "            label=\"ğŸ“ SÃ©lectionnez vos documents\",\n",
    "            file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".json\", \".csv\"]\n",
    "        )\n",
    "        upload_btn = gr.Button(\"ğŸš€ Upload et Traiter\", variant=\"primary\")\n",
    "        upload_output = gr.Textbox(\n",
    "            label=\"ğŸ“Š RÃ©sultat\",\n",
    "            lines=10,\n",
    "            placeholder=\"Les rÃ©sultats d'upload apparaÃ®tront ici...\"\n",
    "        )\n",
    "        \n",
    "        upload_btn.click(\n",
    "            upload_documents,\n",
    "            inputs=file_input,\n",
    "            outputs=upload_output\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"ğŸ’¬ Ask Questions\"):\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### Posez vos questions\n",
    "            \n",
    "            Le systÃ¨me va:\n",
    "            1. ğŸ” Rechercher les passages pertinents\n",
    "            2. ğŸ”’ **Filtrer automatiquement les donnÃ©es privÃ©es**\n",
    "            3. ğŸ§  GÃ©nÃ©rer une rÃ©ponse avec Mistral (uniquement donnÃ©es publiques)\n",
    "            4. ğŸ“š Citer les sources utilisÃ©es\n",
    "            \n",
    "            âš ï¸ Si la rÃ©ponse nÃ©cessite des donnÃ©es privÃ©es, vous verrez : **\"DÃ©solÃ©, donnÃ©e confidentielle\"**\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        question_input = gr.Textbox(\n",
    "            label=\"â“ Votre question\",\n",
    "            placeholder=\"Ex: Quels sont les objectifs de GreenPower pour 2025?\",\n",
    "            lines=3\n",
    "        )\n",
    "        \n",
    "        top_k_slider = gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=10,\n",
    "            value=3,\n",
    "            step=1,\n",
    "            label=\"ğŸ¯ Nombre de chunks Ã  rÃ©cupÃ©rer\",\n",
    "            info=\"Plus de chunks = plus de contexte (mais plus lent)\"\n",
    "        )\n",
    "        \n",
    "        ask_btn = gr.Button(\"ğŸ¤” Obtenir la RÃ©ponse\", variant=\"primary\")\n",
    "        \n",
    "        answer_output = gr.Markdown(\n",
    "            label=\"ğŸ’¡ RÃ©ponse\",\n",
    "            value=\"*La rÃ©ponse apparaÃ®tra ici...*\"\n",
    "        )\n",
    "        \n",
    "        ask_btn.click(\n",
    "            search_and_answer,\n",
    "            inputs=[question_input, top_k_slider],\n",
    "            outputs=answer_output\n",
    "        )\n",
    "        \n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"Quels sont les objectifs principaux de l'entreprise?\", 3],\n",
    "                [\"Quelle est la stratÃ©gie de dÃ©veloppement?\", 5],\n",
    "                [\"Quels sont les projets en cours?\", 3],\n",
    "            ],\n",
    "            inputs=[question_input, top_k_slider],\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"â„¹ï¸ Info\"):\n",
    "        gr.Markdown(\n",
    "            f\"\"\"\n",
    "            ### ğŸ”§ Configuration Technique\n",
    "            \n",
    "            - **Vector DB:** Qdrant ({QDRANT_URL})\n",
    "            - **Embeddings:** {EMBEDDING_MODEL}\n",
    "            - **LLM:** Mistral Small\n",
    "            - **Collection:** {COLLECTION_NAME}\n",
    "            - **Chunk size:** {CHUNK_SIZE} caractÃ¨res\n",
    "            - **Overlap:** {CHUNK_OVERLAP} caractÃ¨res\n",
    "            \n",
    "            ### ğŸ”’ SystÃ¨me de ConfidentialitÃ©\n",
    "            \n",
    "            **Comment Ã§a marche:**\n",
    "            1. Les fichiers avec prÃ©fixe `private_` sont dÃ©tectÃ©s Ã  l'upload\n",
    "            2. Chaque chunk reÃ§oit un flag `is_private: true/false` dans Qdrant\n",
    "            3. Lors de la recherche, les chunks privÃ©s sont **automatiquement filtrÃ©s**\n",
    "            4. Si tous les rÃ©sultats sont privÃ©s â†’ Message \"DonnÃ©e confidentielle\"\n",
    "            \n",
    "            **Exemple d'utilisation:**\n",
    "            ```\n",
    "            Documents:\n",
    "            - rapport_2024.pdf â†’ PUBLIC âœ…\n",
    "            - private_salaires.pdf â†’ PRIVÃ‰ ğŸ”’\n",
    "            - objectifs.docx â†’ PUBLIC âœ…\n",
    "            - private_contrats_clients.csv â†’ PRIVÃ‰ ğŸ”’\n",
    "            \n",
    "            Question: \"Quels sont nos objectifs?\"\n",
    "            â†’ RÃ©ponse basÃ©e uniquement sur rapport_2024.pdf et objectifs.docx\n",
    "            \n",
    "            Question: \"Quels sont les salaires de l'Ã©quipe?\"\n",
    "            â†’ \"DÃ©solÃ©, donnÃ©e confidentielle\" ğŸ”’\n",
    "            ```\n",
    "            \n",
    "            ### ğŸ“ Notes\n",
    "            \n",
    "            - Mode in-memory ne persiste pas les donnÃ©es entre redÃ©marrages\n",
    "            - Pour la production, utilisez Qdrant en mode serveur ou cloud\n",
    "            - Les embeddings sont gÃ©nÃ©rÃ©s localement (CPU)\n",
    "            \n",
    "            ### âœ¨ NouveautÃ©s v05\n",
    "            \n",
    "            - âœ… DÃ©tection automatique des fichiers `private_*`\n",
    "            - âœ… Flag `is_private` dans les mÃ©tadonnÃ©es Qdrant\n",
    "            - âœ… Filtrage intelligent lors de la recherche\n",
    "            - âœ… Messages de confidentialitÃ© transparents\n",
    "            - âœ… Compteur de rÃ©sultats privÃ©s exclus\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "# Lancer l'interface\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ Lancement de l'interface Gradio v05 - Private Filter...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "demo.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=7651,\n",
    "    share=False,\n",
    "    show_error=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
