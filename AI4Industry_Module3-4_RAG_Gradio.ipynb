{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéì AI4Industry - Module 3 & 4 : RAG Vectoriel\n",
        "## Mensaflow ¬© 2025 - Formation CNAM\n",
        "\n",
        "**Objectifs :**\n",
        "- ‚úÖ Comprendre le fonctionnement d'un RAG simple\n",
        "- ‚úÖ Tester avec les fake datas GreenPower Solutions\n",
        "- ‚úÖ Interface Gradio pour import de documents\n",
        "- ‚úÖ Chatbot conversationnel\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Installation des d√©pendances"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installation Ollama\n",
        "!sudo apt-get install -y zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Installation des packages Python avec versions compatibles\n",
        "!pip install -q langchain==0.3.20 langchain-community==0.3.20 langchain-core==0.3.40\n",
        "!pip install -q qdrant-client==1.7.0\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q langchain-ollama\n",
        "!pip install -q gradio\n",
        "!pip install -q pypdf\n",
        "!pip install -q python-docx\n",
        "\n",
        "print(\"‚úÖ Toutes les d√©pendances install√©es !\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ D√©marrage du serveur Ollama"
      ],
      "metadata": {
        "id": "ollama_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# D√©marrer Ollama en arri√®re-plan\n",
        "print(\"üîÑ D√©marrage du serveur Ollama...\")\n",
        "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(5)\n",
        "\n",
        "# T√©l√©charger Mistral 7B\n",
        "print(\"üì• T√©l√©chargement de Mistral 7B (peut prendre 2-3 minutes)...\")\n",
        "!ollama pull mistral\n",
        "print(\"‚úÖ Mistral pr√™t !\")"
      ],
      "metadata": {
        "id": "start_ollama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Configuration du syst√®me RAG"
      ],
      "metadata": {
        "id": "rag_config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "# 1. Initialiser le LLM\n",
        "print(\"ü§ñ Initialisation de Mistral...\")\n",
        "llm = OllamaLLM(\n",
        "    model=\"mistral\",\n",
        "    base_url=\"http://localhost:11434\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# 2. Initialiser les embeddings\n",
        "print(\"üìä Chargement du mod√®le d'embeddings...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "# 3. Initialiser Qdrant\n",
        "print(\"üóÑÔ∏è Initialisation de Qdrant...\")\n",
        "qdrant_client = QdrantClient(location=\":memory:\")\n",
        "collection_name = \"greenpower_docs\"\n",
        "\n",
        "# Cr√©er la collection\n",
        "qdrant_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
        ")\n",
        "\n",
        "# 4. Cr√©er le vectorstore\n",
        "vectorstore = Qdrant(\n",
        "    client=qdrant_client,\n",
        "    collection_name=collection_name,\n",
        "    embeddings=embeddings\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Syst√®me RAG initialis√© !\")"
      ],
      "metadata": {
        "id": "init_rag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÑ Fonctions de traitement de documents"
      ],
      "metadata": {
        "id": "doc_processing_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from PyPDF2 import PdfReader\n",
        "import docx\n",
        "import io\n",
        "\n",
        "# Text splitter pour chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "def process_pdf(file):\n",
        "    \"\"\"Extraire texte d'un PDF\"\"\"\n",
        "    try:\n",
        "        pdf_reader = PdfReader(io.BytesIO(file))\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Erreur PDF: {str(e)}\"\n",
        "\n",
        "def process_docx(file):\n",
        "    \"\"\"Extraire texte d'un DOCX\"\"\"\n",
        "    try:\n",
        "        doc = docx.Document(io.BytesIO(file))\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Erreur DOCX: {str(e)}\"\n",
        "\n",
        "def process_txt(file):\n",
        "    \"\"\"Extraire texte d'un TXT\"\"\"\n",
        "    try:\n",
        "        text = file.decode('utf-8')\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Erreur TXT: {str(e)}\"\n",
        "\n",
        "def add_documents_to_vectorstore(files, progress=None):\n",
        "    \"\"\"Ajouter des documents au vectorstore\"\"\"\n",
        "    if not files:\n",
        "        return \"‚ö†Ô∏è Aucun fichier s√©lectionn√©\"\n",
        "    \n",
        "    all_documents = []\n",
        "    status_messages = []\n",
        "    \n",
        "    for i, file in enumerate(files):\n",
        "        try:\n",
        "            filename = file.name if hasattr(file, 'name') else f\"file_{i}\"\n",
        "            \n",
        "            # Lire le contenu du fichier\n",
        "            file_content = file.read() if hasattr(file, 'read') else file\n",
        "            \n",
        "            # Extraire le texte selon le type\n",
        "            if filename.endswith('.pdf'):\n",
        "                text = process_pdf(file_content)\n",
        "            elif filename.endswith('.docx'):\n",
        "                text = process_docx(file_content)\n",
        "            elif filename.endswith('.txt'):\n",
        "                text = process_txt(file_content)\n",
        "            else:\n",
        "                status_messages.append(f\"‚ö†Ô∏è {filename}: Format non support√©\")\n",
        "                continue\n",
        "            \n",
        "            if text.startswith(\"Erreur\"):\n",
        "                status_messages.append(f\"‚ùå {filename}: {text}\")\n",
        "                continue\n",
        "            \n",
        "            # Cr√©er un document\n",
        "            doc = Document(\n",
        "                page_content=text,\n",
        "                metadata={\"source\": filename}\n",
        "            )\n",
        "            \n",
        "            # D√©couper en chunks\n",
        "            chunks = text_splitter.split_documents([doc])\n",
        "            all_documents.extend(chunks)\n",
        "            \n",
        "            status_messages.append(f\"‚úÖ {filename}: {len(chunks)} chunks cr√©√©s\")\n",
        "            \n",
        "            if progress:\n",
        "                progress((i + 1) / len(files))\n",
        "                \n",
        "        except Exception as e:\n",
        "            status_messages.append(f\"‚ùå {filename}: {str(e)}\")\n",
        "    \n",
        "    # Ajouter tous les documents au vectorstore\n",
        "    if all_documents:\n",
        "        try:\n",
        "            vectorstore.add_documents(all_documents)\n",
        "            status_messages.append(f\"\\nüéâ Total: {len(all_documents)} chunks index√©s dans Qdrant\")\n",
        "        except Exception as e:\n",
        "            status_messages.append(f\"\\n‚ùå Erreur d'indexation: {str(e)}\")\n",
        "    \n",
        "    return \"\\n\".join(status_messages)\n",
        "\n",
        "print(\"‚úÖ Fonctions de traitement de documents pr√™tes\")"
      ],
      "metadata": {
        "id": "doc_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí¨ Fonction de Chat avec RAG"
      ],
      "metadata": {
        "id": "chat_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_rag(message, history):\n",
        "    \"\"\"Fonction de chat avec contexte RAG\"\"\"\n",
        "    if not message.strip():\n",
        "        return \"‚ö†Ô∏è Veuillez entrer une question\"\n",
        "    \n",
        "    try:\n",
        "        # 1. Rechercher les documents pertinents\n",
        "        relevant_docs = vectorstore.similarity_search(message, k=3)\n",
        "        \n",
        "        if not relevant_docs:\n",
        "            return \"‚ö†Ô∏è Aucun document n'a √©t√© index√©. Veuillez d'abord importer des documents.\"\n",
        "        \n",
        "        # 2. Construire le contexte\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1} ({doc.metadata.get('source', 'N/A')}):\\n{doc.page_content}\" \n",
        "                                for i, doc in enumerate(relevant_docs)])\n",
        "        \n",
        "        # 3. Construire le prompt\n",
        "        prompt = f\"\"\"Tu es un assistant IA sp√©cialis√© dans les produits GreenPower Solutions.\n",
        "\n",
        "Contexte des documents:\n",
        "{context}\n",
        "\n",
        "Question de l'utilisateur: {message}\n",
        "\n",
        "Instructions:\n",
        "- R√©ponds uniquement en te basant sur le contexte fourni\n",
        "- Si l'information n'est pas dans le contexte, dis-le clairement\n",
        "- Sois pr√©cis et concis\n",
        "- Cite la source si pertinent\n",
        "\n",
        "R√©ponse:\"\"\"\n",
        "        \n",
        "        # 4. Obtenir la r√©ponse du LLM\n",
        "        response = llm.invoke(prompt)\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Erreur: {str(e)}\"\n",
        "\n",
        "print(\"‚úÖ Fonction de chat pr√™te\")"
      ],
      "metadata": {
        "id": "chat_function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üé® Interface Gradio"
      ],
      "metadata": {
        "id": "gradio_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# CSS personnalis√©\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    font-family: 'Arial', sans-serif;\n",
        "}\n",
        ".header {\n",
        "    text-align: center;\n",
        "    padding: 20px;\n",
        "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "    color: white;\n",
        "    border-radius: 10px;\n",
        "    margin-bottom: 20px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Cr√©er l'interface\n",
        "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
        "    \n",
        "    # Header\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"header\">\n",
        "        <h1>üéì AI4Industry - RAG Vectoriel</h1>\n",
        "        <h3>Mensaflow ¬© 2025 - Formation CNAM</h3>\n",
        "        <p>Module 3 & 4 : Chatbot avec RAG + GreenPower Solutions</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        # Colonne gauche : Import de documents\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## üìÇ Import de Documents\")\n",
        "            gr.Markdown(\"\"\"\n",
        "            **Formats support√©s:**\n",
        "            - üìÑ PDF (.pdf)\n",
        "            - üìù Texte (.txt)\n",
        "            - üìÉ Word (.docx)\n",
        "            \"\"\")\n",
        "            \n",
        "            file_input = gr.File(\n",
        "                label=\"S√©lectionnez vos documents\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\", \".txt\", \".docx\"]\n",
        "            )\n",
        "            \n",
        "            upload_btn = gr.Button(\"üì§ Indexer les documents\", variant=\"primary\")\n",
        "            upload_status = gr.Textbox(\n",
        "                label=\"Status d'indexation\",\n",
        "                lines=10,\n",
        "                interactive=False\n",
        "            )\n",
        "            \n",
        "            gr.Markdown(\"\"\"\n",
        "            ---\n",
        "            ### üìä Statistiques\n",
        "            \"\"\")\n",
        "            \n",
        "            stats_btn = gr.Button(\"üîç Afficher statistiques\")\n",
        "            stats_output = gr.Textbox(\n",
        "                label=\"Statistiques Qdrant\",\n",
        "                lines=5,\n",
        "                interactive=False\n",
        "            )\n",
        "        \n",
        "        # Colonne droite : Chat\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"## üí¨ Chat avec RAG\")\n",
        "            gr.Markdown(\"\"\"\n",
        "            Posez vos questions sur les documents GreenPower Solutions index√©s.\n",
        "            Le syst√®me utilisera le RAG pour trouver les informations pertinentes.\n",
        "            \"\"\")\n",
        "            \n",
        "            chatbot = gr.Chatbot(\n",
        "                height=500,\n",
        "                label=\"Assistant GreenPower\",\n",
        "                avatar_images=(None, \"ü§ñ\")\n",
        "            )\n",
        "            \n",
        "            msg = gr.Textbox(\n",
        "                label=\"Votre question\",\n",
        "                placeholder=\"Ex: Quels sont les produits GreenPower disponibles ?\",\n",
        "                lines=2\n",
        "            )\n",
        "            \n",
        "            with gr.Row():\n",
        "                submit_btn = gr.Button(\"üöÄ Envoyer\", variant=\"primary\")\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
        "            \n",
        "            gr.Markdown(\"\"\"\n",
        "            ---\n",
        "            ### üí° Questions sugg√©r√©es:\n",
        "            - Quels sont les produits disponibles ?\n",
        "            - Quelle est la capacit√© batterie du PG-M01 ?\n",
        "            - Quel syst√®me convient pour un festival de 3 jours ?\n",
        "            - Combien co√ªte le PG-P01 ?\n",
        "            \"\"\")\n",
        "    \n",
        "    # Footer\n",
        "    gr.HTML(\"\"\"\n",
        "    <div style=\"text-align: center; padding: 20px; color: #666;\">\n",
        "        <p><strong>Stack Technique:</strong> Mistral 7B (Ollama) + Qdrant + LangChain + Gradio</p>\n",
        "        <p>D√©velopp√© par Mensaflow pour AI4Industry CNAM 2025</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "    \n",
        "    # √âv√©nements\n",
        "    def get_stats():\n",
        "        try:\n",
        "            collection_info = qdrant_client.get_collection(collection_name)\n",
        "            return f\"\"\"üìä Statistiques Qdrant:\n",
        "            \n",
        "üì¶ Collection: {collection_name}\n",
        "üìù Nombre de vectors: {collection_info.vectors_count}\n",
        "üìè Dimension: 384\n",
        "üìê Distance: Cosine\n",
        "‚úÖ Status: Op√©rationnel\n",
        "\"\"\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Erreur: {str(e)}\"\n",
        "    \n",
        "    # Lier les √©v√©nements\n",
        "    upload_btn.click(\n",
        "        fn=add_documents_to_vectorstore,\n",
        "        inputs=[file_input],\n",
        "        outputs=[upload_status]\n",
        "    )\n",
        "    \n",
        "    stats_btn.click(\n",
        "        fn=get_stats,\n",
        "        outputs=[stats_output]\n",
        "    )\n",
        "    \n",
        "    msg.submit(\n",
        "        fn=chat_with_rag,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[chatbot]\n",
        "    ).then(\n",
        "        lambda: \"\",\n",
        "        outputs=[msg]\n",
        "    )\n",
        "    \n",
        "    submit_btn.click(\n",
        "        fn=chat_with_rag,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[chatbot]\n",
        "    ).then(\n",
        "        lambda: \"\",\n",
        "        outputs=[msg]\n",
        "    )\n",
        "    \n",
        "    clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Lancer l'interface\n",
        "print(\"üöÄ Lancement de l'interface Gradio...\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "gradio_interface"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìö Instructions pour les √©tudiants\n",
        "\n",
        "### √âtape 1 : Importer les documents GreenPower\n",
        "1. T√©l√©chargez les fake datas GreenPower depuis le dossier partag√©\n",
        "2. Dans la colonne de gauche, cliquez sur \"S√©lectionnez vos documents\"\n",
        "3. S√©lectionnez plusieurs fichiers (PDF, TXT, DOCX)\n",
        "4. Cliquez sur \"üì§ Indexer les documents\"\n",
        "5. Attendez le message de confirmation\n",
        "\n",
        "### √âtape 2 : Tester le RAG\n",
        "1. Dans la zone de chat, posez une question sur GreenPower\n",
        "2. Observez comment le syst√®me :\n",
        "   - Recherche les documents pertinents\n",
        "   - Construit le contexte\n",
        "   - G√©n√®re une r√©ponse avec Mistral\n",
        "\n",
        "### √âtape 3 : Analyser les performances\n",
        "1. Cliquez sur \"üîç Afficher statistiques\"\n",
        "2. Notez le nombre de chunks index√©s\n",
        "3. Testez diff√©rentes questions et observez la qualit√© des r√©ponses\n",
        "\n",
        "### Questions √† explorer :\n",
        "- Le RAG trouve-t-il toujours les bonnes informations ?\n",
        "- Que se passe-t-il si on pose une question hors contexte ?\n",
        "- Comment am√©liorer la qualit√© du chunking ?\n",
        "- Quelles sont les limites du RAG vectoriel simple ?\n",
        "\n",
        "---\n",
        "## üéØ Objectifs p√©dagogiques atteints\n",
        "\n",
        "‚úÖ Comprendre l'architecture d'un RAG  \n",
        "‚úÖ Ma√Ætriser le chunking de documents  \n",
        "‚úÖ Utiliser une base vectorielle (Qdrant)  \n",
        "‚úÖ Int√©grer un LLM local (Ollama + Mistral)  \n",
        "‚úÖ Cr√©er une interface utilisateur (Gradio)  \n",
        "\n",
        "**Prochaine √©tape :** Module 5 - GraphRAG pour d√©passer les limites du RAG vectoriel !\n",
        "\n",
        "---\n",
        "**Mensaflow ¬© 2025 - Formation AI4Industry CNAM**"
      ],
      "metadata": {
        "id": "instructions"
      }
    }
  ]
}
