{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9bf820",
   "metadata": {},
   "source": [
    "# GreenPower RAG System v05 - Content-Based Private Filter\n",
    "\n",
    "## Pipeline complet:\n",
    "1. Setup Qdrant (local vectoriel)\n",
    "2. Chargement documents GreenPower via Gradio\n",
    "3. Chunking + Embeddings (sentence-transformers)\n",
    "4. Stockage Qdrant vector index\n",
    "5. RAG pipeline : query â†’ retrieval â†’ **scan content for private_** â†’ filter â†’ Mistral\n",
    "6. Interface Gradio interactive\n",
    "\n",
    "## NouveautÃ©s v05:\n",
    "- âœ¨ **Filtrage par contenu**: DÃ©tecte les valeurs `private_*` DANS le texte des chunks\n",
    "- âœ¨ **Analyse post-retrieval**: Scanne chaque chunk retournÃ© par Qdrant\n",
    "- âœ¨ **Message de confidentialitÃ©**: \"DÃ©solÃ©, donnÃ©e confidentielle\" si chunks contiennent private_\n",
    "- âœ¨ **Pattern matching**: Regex pour dÃ©tecter tout pattern `private_xxx`\n",
    "\n",
    "## Comment Ã§a marche:\n",
    "- Un document normal peut contenir des rÃ©fÃ©rences Ã  des donnÃ©es privÃ©es\n",
    "- Exemple: `\"Le client private_client_001 a commandÃ©...\"`\n",
    "- Le systÃ¨me dÃ©tecte `private_client_001` et filtre ce chunk automatiquement\n",
    "- L'utilisateur ne verra jamais ces informations sensibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Installation des dÃ©pendances\n",
    "!pip install -q langchain-mistralai langchain-community langchain-text-splitters\n",
    "!pip install -q qdrant-client gradio sentence-transformers\n",
    "!pip install -q pypdf python-docx python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "from typing import List, Tuple\n",
    "import pypdf\n",
    "import docx\n",
    "import json\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = Path('.env')\n",
    "if env_path.exists():\n",
    "    with open(env_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#') and '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "QDRANT_URL = os.getenv('QDRANT_URL', ':memory:')  # Use :memory: for local or cloud URL\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY', None)\n",
    "\n",
    "# ğŸ”’ PATTERN POUR DÃ‰TECTER LES DONNÃ‰ES PRIVÃ‰ES\n",
    "PRIVATE_PATTERN = re.compile(r'private_\\w+', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_clients",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialisation des clients\n",
    "\n",
    "# Configuration CHUNKS\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Initialize components\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "llm = ChatMistralAI(model='mistral-small-latest', mistral_api_key=MISTRAL_API_KEY, temperature=0.7)\n",
    "\n",
    "qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "COLLECTION_NAME = \"greenpower_docs\"\n",
    "\n",
    "# Text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "print(\"âœ… Clients initialisÃ©s\")\n",
    "print(f\"   - Qdrant: Mode {QDRANT_URL}\")\n",
    "print(f\"   - Embeddings: {EMBEDDING_MODEL}\")\n",
    "print(f\"   - LLM: Mistral Small\")\n",
    "print(f\"   - ğŸ”’ Pattern privÃ©: {PRIVATE_PATTERN.pattern}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: CrÃ©ation de la collection Qdrant\n",
    "\n",
    "# Obtenir la dimension des embeddings en gÃ©nÃ©rant un embedding de test\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "VECTOR_SIZE = len(test_embedding)\n",
    "\n",
    "def create_collection_if_not_exists():\n",
    "    \"\"\"CrÃ©e la collection Qdrant si elle n'existe pas dÃ©jÃ \"\"\"\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections()\n",
    "        collection_names = [c.name for c in collections.collections]\n",
    "        \n",
    "        if COLLECTION_NAME in collection_names:\n",
    "            print(f\"â„¹ï¸ Collection '{COLLECTION_NAME}' existe dÃ©jÃ \")\n",
    "            return\n",
    "        \n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(\n",
    "                size=VECTOR_SIZE,\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        print(f\"âœ… Collection '{COLLECTION_NAME}' crÃ©Ã©e (dimension: {VECTOR_SIZE})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur crÃ©ation collection: {e}\")\n",
    "        raise\n",
    "\n",
    "# CrÃ©er la collection\n",
    "create_collection_if_not_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Fonctions utilitaires\n",
    "\n",
    "def extract_text_from_file(file_path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF, DOCX, TXT, JSON ou CSV\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    try:\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            with open(file_path, 'rb') as f:\n",
    "                pdf_reader = pypdf.PdfReader(f)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        \n",
    "        elif file_path.suffix.lower() in ['.docx', '.doc']:\n",
    "            doc = docx.Document(file_path)\n",
    "            return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.json':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    text_parts = []\n",
    "                    for key, value in data.items():\n",
    "                        if isinstance(value, (dict, list)):\n",
    "                            text_parts.append(f\"{key}: {json.dumps(value, ensure_ascii=False, indent=2)}\")\n",
    "                        else:\n",
    "                            text_parts.append(f\"{key}: {value}\")\n",
    "                    return \"\\n\".join(text_parts)\n",
    "                elif isinstance(data, list):\n",
    "                    return \"\\n\\n\".join([json.dumps(item, ensure_ascii=False, indent=2) for item in data])\n",
    "                else:\n",
    "                    return str(data)\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.csv':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                csv_reader = csv.DictReader(f)\n",
    "                rows = []\n",
    "                for row in csv_reader:\n",
    "                    row_text = \", \".join([f\"{key}: {value}\" for key, value in row.items()])\n",
    "                    rows.append(row_text)\n",
    "                return \"\\n\".join(rows)\n",
    "        \n",
    "        else:\n",
    "            return f\"Format non supportÃ©: {file_path.suffix}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Erreur lecture fichier {file_path.name}: {str(e)}\"\n",
    "\n",
    "def contains_private_data(text: str) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"ğŸ”’ DÃ©tecte si le texte contient des valeurs privÃ©es (private_xxx)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[bool, List[str]]: (has_private, list_of_private_values)\n",
    "    \"\"\"\n",
    "    matches = PRIVATE_PATTERN.findall(text)\n",
    "    return len(matches) > 0, matches\n",
    "\n",
    "def chunk_and_embed(text: str, source: str) -> List[dict]:\n",
    "    \"\"\"DÃ©coupe le texte en chunks et gÃ©nÃ¨re les embeddings\"\"\"\n",
    "    # Chunking\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Embeddings\n",
    "    chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "    \n",
    "    # PrÃ©parer les points pour Qdrant\n",
    "    points = []\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        point = PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector=embedding,\n",
    "            payload={\n",
    "                \"text\": chunk,\n",
    "                \"source\": source,\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    return points\n",
    "\n",
    "print(\"âœ… Fonctions utilitaires dÃ©finies (avec dÃ©tection private_ dans le contenu)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Fonction d'upload de documents\n",
    "\n",
    "def upload_documents(files):\n",
    "    \"\"\"Traite et stocke les documents uploadÃ©s\"\"\"\n",
    "    if not files:\n",
    "        return \"âŒ Aucun fichier uploadÃ©\"\n",
    "    \n",
    "    # S'assurer que la collection existe\n",
    "    create_collection_if_not_exists()\n",
    "    \n",
    "    total_chunks = 0\n",
    "    chunks_with_private_content = 0\n",
    "    results = []\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            # Extraction du texte\n",
    "            file_path = file.name if hasattr(file, 'name') else file\n",
    "            filename = Path(file_path).name\n",
    "            \n",
    "            text = extract_text_from_file(file_path)\n",
    "            \n",
    "            if text.startswith(\"Erreur\") or text.startswith(\"Format non supportÃ©\"):\n",
    "                results.append(f\"âš ï¸ {filename}: {text}\")\n",
    "                continue\n",
    "            \n",
    "            # Chunking et embedding\n",
    "            points = chunk_and_embed(text, filename)\n",
    "            \n",
    "            # ğŸ” Analyser combien de chunks contiennent des donnÃ©es privÃ©es\n",
    "            private_chunks_in_doc = 0\n",
    "            for point in points:\n",
    "                has_private, _ = contains_private_data(point.payload[\"text\"])\n",
    "                if has_private:\n",
    "                    private_chunks_in_doc += 1\n",
    "            \n",
    "            # Stockage dans Qdrant\n",
    "            qdrant_client.upsert(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                points=points\n",
    "            )\n",
    "            \n",
    "            total_chunks += len(points)\n",
    "            chunks_with_private_content += private_chunks_in_doc\n",
    "            \n",
    "            if private_chunks_in_doc > 0:\n",
    "                results.append(\n",
    "                    f\"âœ… {filename}: {len(points)} chunks \"\n",
    "                    f\"(âš ï¸ {private_chunks_in_doc} contiennent des donnÃ©es private_)\"\n",
    "                )\n",
    "            else:\n",
    "                results.append(f\"âœ… {filename}: {len(points)} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append(f\"âŒ {filename}: Erreur - {str(e)}\")\n",
    "    \n",
    "    summary = f\"\\n\\nğŸ“Š **Total: {total_chunks} chunks** stockÃ©s dans '{COLLECTION_NAME}'\\n\"\n",
    "    if chunks_with_private_content > 0:\n",
    "        summary += f\"âš ï¸ **{chunks_with_private_content} chunks** contiennent des valeurs private_* \"\n",
    "        summary += \"(seront filtrÃ©s lors des recherches)\"\n",
    "    \n",
    "    return \"\\n\".join(results) + summary\n",
    "\n",
    "print(\"âœ… Fonction upload_documents dÃ©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Fonction RAG avec filtrage des contenus privÃ©s\n",
    "\n",
    "def search_and_answer(question: str, top_k: int = 3):\n",
    "    \"\"\"ğŸ”’ Recherche dans Qdrant et gÃ©nÃ¨re une rÃ©ponse (filtre les chunks avec private_*)\"\"\"\n",
    "    if not question or question.strip() == \"\":\n",
    "        return \"âš ï¸ Veuillez poser une question\"\n",
    "    \n",
    "    try:\n",
    "        # 1. GÃ©nÃ©rer l'embedding de la question\n",
    "        question_embedding = embeddings.embed_query(question)\n",
    "        \n",
    "        # 2. Recherche dans Qdrant avec un top_k plus large pour compenser le filtrage\n",
    "        search_results = qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=question_embedding,\n",
    "            limit=top_k * 5  # Chercher 5x plus pour avoir assez aprÃ¨s filtrage\n",
    "        )\n",
    "        \n",
    "        if not search_results:\n",
    "            return \"âš ï¸ Aucun document trouvÃ©. Uploadez d'abord des documents.\"\n",
    "        \n",
    "        # 3. ğŸ”’ FILTRAGE DES CHUNKS CONTENANT DES DONNÃ‰ES PRIVÃ‰ES\n",
    "        filtered_results = []\n",
    "        private_chunks = []\n",
    "        private_values_found = set()\n",
    "        \n",
    "        for point in search_results:\n",
    "            chunk_text = point.payload[\"text\"]\n",
    "            \n",
    "            # VÃ©rifier si le chunk contient des valeurs private_\n",
    "            has_private, private_matches = contains_private_data(chunk_text)\n",
    "            \n",
    "            if has_private:\n",
    "                # ğŸš« Ce chunk contient des donnÃ©es privÃ©es, on le stocke mais ne l'utilise pas\n",
    "                private_chunks.append({\n",
    "                    'source': point.payload.get('source', 'Unknown'),\n",
    "                    'matches': private_matches\n",
    "                })\n",
    "                private_values_found.update(private_matches)\n",
    "                continue\n",
    "            \n",
    "            # âœ… Chunk clean, on l'ajoute\n",
    "            filtered_results.append(point)\n",
    "            \n",
    "            # Limiter au top_k demandÃ©\n",
    "            if len(filtered_results) >= top_k:\n",
    "                break\n",
    "        \n",
    "        # 4. VÃ©rifier s'il reste des rÃ©sultats aprÃ¨s filtrage\n",
    "        if not filtered_results:\n",
    "            # Tous les rÃ©sultats contenaient des donnÃ©es privÃ©es\n",
    "            return f\"\"\"## ğŸ”’ DonnÃ©e confidentielle\n",
    "\n",
    "DÃ©solÃ©, les informations pertinentes pour cette question contiennent des **donnÃ©es confidentielles** et ne peuvent pas Ãªtre affichÃ©es.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ”’ **{len(private_chunks)} chunks** ont Ã©tÃ© trouvÃ©s mais contenaient des valeurs privÃ©es :\n",
    "{', '.join(sorted(private_values_found))}\n",
    "\n",
    "ğŸ’¡ Ces donnÃ©es sont protÃ©gÃ©es et ne seront jamais affichÃ©es dans les rÃ©ponses.\n",
    "\"\"\"\n",
    "        \n",
    "        # 5. Extraire les chunks pertinents (uniquement ceux sans donnÃ©es privÃ©es)\n",
    "        contexts = []\n",
    "        sources = []\n",
    "        for point in filtered_results:\n",
    "            contexts.append(point.payload[\"text\"])\n",
    "            source = point.payload.get(\"source\", \"Unknown\")\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        \n",
    "        context_text = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "        \n",
    "        # 6. Construire le prompt\n",
    "        prompt = f\"\"\"Tu es un assistant qui rÃ©pond aux questions en te basant UNIQUEMENT sur le contexte fourni.\n",
    "\n",
    "Contexte:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- RÃ©ponds de maniÃ¨re claire et concise\n",
    "- Base-toi UNIQUEMENT sur le contexte fourni\n",
    "- Si l'information n'est pas dans le contexte, dis-le clairement\n",
    "- Cite les sources quand pertinent\n",
    "\n",
    "RÃ©ponse:\"\"\"\n",
    "        \n",
    "        # 7. GÃ©nÃ©rer la rÃ©ponse avec Mistral\n",
    "        response = llm.invoke(prompt)\n",
    "        answer = response.content\n",
    "        \n",
    "        # 8. Formater la rÃ©ponse avec info sur filtrage\n",
    "        output = f\"\"\"## ğŸ’¬ RÃ©ponse\n",
    "\n",
    "{answer}\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ“š **Sources consultÃ©es:** {', '.join(sources)}\n",
    "ğŸ” **{len(contexts)} chunks** analysÃ©s (sans donnÃ©es confidentielles)\n",
    "\"\"\"\n",
    "        \n",
    "        # Ajouter un warning si des chunks privÃ©s ont Ã©tÃ© exclus\n",
    "        if private_chunks:\n",
    "            output += f\"\\n\\nğŸ”’ **{len(private_chunks)} chunks** ont Ã©tÃ© exclus car ils contenaient des valeurs privÃ©es :\"\n",
    "            output += f\"\\n   {', '.join(sorted(private_values_found))}\"\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"âŒ Erreur: {str(e)}\\n\\nDÃ©tails technique: {type(e).__name__}\"\n",
    "\n",
    "print(\"âœ… Fonction search_and_answer dÃ©finie (avec scan de contenu pour private_)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio_interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Interface Gradio\n",
    "\n",
    "with gr.Blocks(title=\"GreenPower RAG v05 - Content Filter\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ğŸŒ± GreenPower RAG System v05 - ğŸ”’ Content-Based Private Filter\n",
    "        \n",
    "        **Retrieval-Augmented Generation** avec **filtrage intelligent des contenus privÃ©s**\n",
    "        \n",
    "        ### ğŸ“‹ Workflow:\n",
    "        1. **Upload** vos documents (PDF, DOCX, TXT, JSON, CSV)\n",
    "        2. Le systÃ¨me indexe tout le contenu dans Qdrant\n",
    "        3. **Ask** vos questions\n",
    "        4. Le systÃ¨me **scanne les rÃ©sultats** et filtre automatiquement les chunks contenant `private_xxx`\n",
    "        \n",
    "        ### ğŸ”’ Protection des donnÃ©es:\n",
    "        - DÃ©tection automatique de patterns `private_*` dans le **contenu des chunks**\n",
    "        - Ex: `private_client_001`, `private_salary_data`, `private_contract_123`\n",
    "        - Ces chunks ne seront **jamais affichÃ©s** mÃªme si pertinents pour la question\n",
    "        \n",
    "        ### ğŸ’¡ Exemple:\n",
    "        ```\n",
    "        Document contient:\n",
    "        \"Notre objectif 2025 est de croÃ®tre de 20%.\"\n",
    "        \"Le client private_client_001 reprÃ©sente 15% du CA.\"\n",
    "        \n",
    "        Question: \"Quels sont les objectifs 2025?\"\n",
    "        â†’ âœ… RÃ©pond avec le premier chunk\n",
    "        â†’ ğŸ”’ Ignore le second (contient private_client_001)\n",
    "        ```\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Tab(\"ğŸ“¤ Upload Documents\"):\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### Upload vos documents\n",
    "            \n",
    "            Formats supportÃ©s: **PDF**, **DOCX**, **TXT**, **JSON**, **CSV**\n",
    "            \n",
    "            ğŸ”’ **DonnÃ©es privÃ©es dans le contenu:**\n",
    "            - Utilisez le pattern `private_xxx` pour marquer des valeurs sensibles\n",
    "            - Ex: `\"Le projet private_project_alpha coÃ»te 2Mâ‚¬\"`\n",
    "            - Ces rÃ©fÃ©rences seront automatiquement dÃ©tectÃ©es et filtrÃ©es\n",
    "            \n",
    "            Les documents seront:\n",
    "            - DÃ©coupÃ©s en chunks de 500 caractÃ¨res\n",
    "            - VectorisÃ©s avec sentence-transformers\n",
    "            - StockÃ©s dans Qdrant\n",
    "            - ScannÃ©s pour dÃ©tecter les valeurs private_*\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        file_input = gr.File(\n",
    "            file_count=\"multiple\",\n",
    "            label=\"ğŸ“ SÃ©lectionnez vos documents\",\n",
    "            file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".json\", \".csv\"]\n",
    "        )\n",
    "        upload_btn = gr.Button(\"ğŸš€ Upload et Traiter\", variant=\"primary\")\n",
    "        upload_output = gr.Textbox(\n",
    "            label=\"ğŸ“Š RÃ©sultat\",\n",
    "            lines=10,\n",
    "            placeholder=\"Les rÃ©sultats d'upload apparaÃ®tront ici...\"\n",
    "        )\n",
    "        \n",
    "        upload_btn.click(\n",
    "            upload_documents,\n",
    "            inputs=file_input,\n",
    "            outputs=upload_output\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"ğŸ’¬ Ask Questions\"):\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### Posez vos questions\n",
    "            \n",
    "            Le systÃ¨me va:\n",
    "            1. ğŸ” Rechercher les passages pertinents dans Qdrant\n",
    "            2. ğŸ”’ **Scanner chaque chunk** pour dÃ©tecter des valeurs `private_*`\n",
    "            3. ğŸš« **Filtrer automatiquement** les chunks contenant des donnÃ©es privÃ©es\n",
    "            4. ğŸ§  GÃ©nÃ©rer une rÃ©ponse avec Mistral (uniquement donnÃ©es publiques)\n",
    "            5. ğŸ“š Citer les sources et indiquer les donnÃ©es filtrÃ©es\n",
    "            \n",
    "            âš ï¸ Si tous les chunks pertinents contiennent `private_*` â†’ **\"DÃ©solÃ©, donnÃ©e confidentielle\"**\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        question_input = gr.Textbox(\n",
    "            label=\"â“ Votre question\",\n",
    "            placeholder=\"Ex: Quels sont les objectifs de GreenPower pour 2025?\",\n",
    "            lines=3\n",
    "        )\n",
    "        \n",
    "        top_k_slider = gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=10,\n",
    "            value=3,\n",
    "            step=1,\n",
    "            label=\"ğŸ¯ Nombre de chunks Ã  rÃ©cupÃ©rer\",\n",
    "            info=\"Plus de chunks = plus de contexte (mais plus lent)\"\n",
    "        )\n",
    "        \n",
    "        ask_btn = gr.Button(\"ğŸ¤” Obtenir la RÃ©ponse\", variant=\"primary\")\n",
    "        \n",
    "        answer_output = gr.Markdown(\n",
    "            label=\"ğŸ’¡ RÃ©ponse\",\n",
    "            value=\"*La rÃ©ponse apparaÃ®tra ici...*\"\n",
    "        )\n",
    "        \n",
    "        ask_btn.click(\n",
    "            search_and_answer,\n",
    "            inputs=[question_input, top_k_slider],\n",
    "            outputs=answer_output\n",
    "        )\n",
    "        \n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"Quels sont les objectifs principaux de l'entreprise?\", 3],\n",
    "                [\"Quelle est la stratÃ©gie de dÃ©veloppement?\", 5],\n",
    "                [\"Quels sont les projets en cours?\", 3],\n",
    "            ],\n",
    "            inputs=[question_input, top_k_slider],\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"â„¹ï¸ Info\"):\n",
    "        gr.Markdown(\n",
    "            f\"\"\"\n",
    "            ### ğŸ”§ Configuration Technique\n",
    "            \n",
    "            - **Vector DB:** Qdrant ({QDRANT_URL})\n",
    "            - **Embeddings:** {EMBEDDING_MODEL}\n",
    "            - **LLM:** Mistral Small\n",
    "            - **Collection:** {COLLECTION_NAME}\n",
    "            - **Chunk size:** {CHUNK_SIZE} caractÃ¨res\n",
    "            - **Overlap:** {CHUNK_OVERLAP} caractÃ¨res\n",
    "            - **ğŸ”’ Pattern privÃ©:** `{PRIVATE_PATTERN.pattern}`\n",
    "            \n",
    "            ### ğŸ”’ SystÃ¨me de ConfidentialitÃ© par Contenu\n",
    "            \n",
    "            **Comment Ã§a marche:**\n",
    "            1. Les documents sont indexÃ©s normalement dans Qdrant (pas de prÃ©-filtrage)\n",
    "            2. Lors d'une recherche, Qdrant retourne les chunks les plus pertinents\n",
    "            3. **Chaque chunk est scannÃ©** avec une regex : `/private_\\\\w+/`\n",
    "            4. Si un chunk contient `private_xxx` â†’ **il est rejetÃ© automatiquement**\n",
    "            5. Seuls les chunks \"clean\" sont envoyÃ©s Ã  Mistral\n",
    "            6. L'utilisateur est informÃ© si des donnÃ©es ont Ã©tÃ© filtrÃ©es\n",
    "            \n",
    "            **Patterns dÃ©tectÃ©s:**\n",
    "            - `private_client_001` âœ…\n",
    "            - `Private_Salary_Data` âœ… (case insensitive)\n",
    "            - `private_contract_alpha` âœ…\n",
    "            - `PRIVATE_PROJECT_X` âœ…\n",
    "            - `privatestuff` âŒ (pas de underscore)\n",
    "            - `my_private_thing` âŒ (ne commence pas par private_)\n",
    "            \n",
    "            **Exemple d'utilisation:**\n",
    "            ```\n",
    "            Document: strategie.txt\n",
    "            ---\n",
    "            Chunk 1: \"Nos objectifs 2025 sont de croÃ®tre de 25%.\"\n",
    "            â†’ âœ… PUBLIC (pas de private_)\n",
    "            \n",
    "            Chunk 2: \"Le client private_client_acme reprÃ©sente 30% du CA.\"\n",
    "            â†’ ğŸ”’ PRIVÃ‰ (dÃ©tectÃ©: private_client_acme)\n",
    "            \n",
    "            Chunk 3: \"Nous lanÃ§ons 3 nouveaux produits en Q2.\"\n",
    "            â†’ âœ… PUBLIC\n",
    "            \n",
    "            Question: \"Quels sont les objectifs?\"\n",
    "            â†’ Utilise Chunks 1 et 3\n",
    "            â†’ Ignore Chunk 2\n",
    "            â†’ Indique \"1 chunk exclu (private_client_acme)\"\n",
    "            ```\n",
    "            \n",
    "            ### ğŸ“ Notes Importantes\n",
    "            \n",
    "            - âš¡ Le filtrage se fait **aprÃ¨s la recherche vectorielle**\n",
    "            - ğŸ¯ Le systÃ¨me cherche 5x plus de chunks pour compenser le filtrage\n",
    "            - ğŸ“Š Les chunks privÃ©s sont comptÃ©s et listÃ©s dans la rÃ©ponse\n",
    "            - ğŸ” Aucune donnÃ©e privÃ©e ne peut \"fuiter\" dans les rÃ©ponses de Mistral\n",
    "            - ğŸ’¾ Mode in-memory ne persiste pas les donnÃ©es entre redÃ©marrages\n",
    "            \n",
    "            ### âœ¨ NouveautÃ©s v05\n",
    "            \n",
    "            - âœ… Regex pattern matching pour `private_*`\n",
    "            - âœ… Scan post-retrieval de tous les chunks\n",
    "            - âœ… Filtrage intelligent avant envoi Ã  Mistral\n",
    "            - âœ… Reporting transparent des valeurs privÃ©es dÃ©tectÃ©es\n",
    "            - âœ… Message de confidentialitÃ© si tous les rÃ©sultats sont privÃ©s\n",
    "            - âœ… Liste des valeurs privÃ©es trouvÃ©es (ex: private_client_001, private_salary_data)\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "# Lancer l'interface\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ Lancement de l'interface Gradio v05 - Content Filter...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "demo.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=7688,\n",
    "    share=False,\n",
    "    show_error=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
