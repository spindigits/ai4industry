{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9bf820",
   "metadata": {},
   "source": [
    "# GreenPower RAG System v04\n",
    "\n",
    "## Pipeline complet:\n",
    "1. Setup Qdrant (local vectoriel)\n",
    "2. Chargement documents GreenPower via Gradio\n",
    "3. Chunking + Embeddings (sentence-transformers)\n",
    "4. Stockage Qdrant vector index\n",
    "5. RAG pipeline : query → retrieval → Mistral\n",
    "6. Interface Gradio interactive\n",
    "\n",
    "## Corrections v04:\n",
    "- Fix méthode search() → query() pour Qdrant\n",
    "- Fix création collection avant upsert\n",
    "- Fix imports langchain\n",
    "- Import JSON et CSV (only txt, docx, txt defore...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "setup_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Installation des dépendances\n",
    "!pip install -q langchain-mistralai langchain-community langchain-text-splitters\n",
    "!pip install -q qdrant-client gradio sentence-transformers\n",
    "!pip install -q pypdf python-docx python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Imports\nimport os\nfrom pathlib import Path\nimport gradio as gr\nfrom langchain_mistralai import ChatMistralAI\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\nimport uuid\nfrom typing import List\nimport pypdf\nimport docx\nimport json\nimport csv\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\nprint(\"Imports OK\")"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = Path('.env')\n",
    "if env_path.exists():\n",
    "    with open(env_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#') and '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "QDRANT_URL = os.getenv('QDRANT_URL', 'URL')  # Use :memory: for local or cloud URL\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_clients",
   "metadata": {},
   "outputs": [],
   "source": "env_path = Path('.env')\nif env_path.exists():\n    with open(env_path) as f:\n        for line in f:\n            line = line.strip()\n            if line and not line.startswith('#') and '=' in line:\n                key, value = line.split('=', 1)\n                os.environ[key.strip()] = value.strip()\n\nMISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\nQDRANT_URL = os.getenv('QDRANT_URL', 'URL')  # Use :memory: for local or cloud URL\nQDRANT_API_KEY = os.getenv('QDRANT_API_KEY', None)\n# Configuration CHUNKS\nCHUNK_SIZE = 500\nCHUNK_OVERLAP = 50\nEMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n# Initialize components\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nllm = ChatMistralAI(model='mistral-small-latest', mistral_api_key=MISTRAL_API_KEY, temperature=0.7)\n\nqdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\nCOLLECTION_NAME = \"greenpower_docs\"\n# Text splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=CHUNK_SIZE,\n    chunk_overlap=CHUNK_OVERLAP,\n    length_function=len,\n)\n\nprint(\"Clients initialises\")\nprint(f\"   - Qdrant: Mode in-memory\")\nprint(f\"   - Embeddings: {EMBEDDING_MODEL}\")\nprint(f\"   - LLM: Mistral Small\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_collection",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Creation de la collection Qdrant\n# Obtenir la dimension des embeddings en generant un embedding de test\ntest_embedding = embeddings.embed_query(\"test\")\nVECTOR_SIZE = len(test_embedding)\n\ndef create_collection_if_not_exists():\n    \"\"\"Cree la collection Qdrant si elle n'existe pas deja\"\"\"\n    try:\n        collections = qdrant_client.get_collections()\n        collection_names = [c.name for c in collections.collections]\n        \n        if COLLECTION_NAME in collection_names:\n            print(f\"[INFO] Collection '{COLLECTION_NAME}' existe deja\")\n            return\n        \n        qdrant_client.create_collection(\n            collection_name=COLLECTION_NAME,\n            vectors_config=VectorParams(\n                size=VECTOR_SIZE,\n                distance=Distance.COSINE\n            )\n        )\n        print(f\"[OK] Collection '{COLLECTION_NAME}' creee (dimension: {VECTOR_SIZE})\")\n    except Exception as e:\n        print(f\"[WARN] Erreur creation collection: {e}\")\n        raise\n\n# Creer la collection\ncreate_collection_if_not_exists()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Fonctions utilitaires\n\ndef extract_text_from_file(file_path: str) -> str:\n    \"\"\"Extrait le texte d'un fichier PDF, DOCX, TXT, JSON ou CSV\"\"\"\n    file_path = Path(file_path)\n    \n    try:\n        if file_path.suffix.lower() == '.pdf':\n            with open(file_path, 'rb') as f:\n                pdf_reader = pypdf.PdfReader(f)\n                text = \"\"\n                for page in pdf_reader.pages:\n                    text += page.extract_text() + \"\\n\"\n                return text\n        \n        elif file_path.suffix.lower() in ['.docx', '.doc']:\n            doc = docx.Document(file_path)\n            return \"\\n\".join([para.text for para in doc.paragraphs])\n        \n        elif file_path.suffix.lower() == '.txt':\n            with open(file_path, 'r', encoding='utf-8') as f:\n                return f.read()\n        \n        elif file_path.suffix.lower() == '.json':\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                # Convertir le JSON en texte lisible\n                if isinstance(data, dict):\n                    # Si c'est un dictionnaire, le formater proprement\n                    text_parts = []\n                    for key, value in data.items():\n                        if isinstance(value, (dict, list)):\n                            text_parts.append(f\"{key}: {json.dumps(value, ensure_ascii=False, indent=2)}\")\n                        else:\n                            text_parts.append(f\"{key}: {value}\")\n                    return \"\\n\".join(text_parts)\n                elif isinstance(data, list):\n                    # Si c'est une liste, traiter chaque element\n                    return \"\\n\\n\".join([json.dumps(item, ensure_ascii=False, indent=2) for item in data])\n                else:\n                    return str(data)\n        \n        elif file_path.suffix.lower() == '.csv':\n            with open(file_path, 'r', encoding='utf-8') as f:\n                csv_reader = csv.DictReader(f)\n                rows = []\n                for row in csv_reader:\n                    # Convertir chaque ligne en texte formate\n                    row_text = \", \".join([f\"{key}: {value}\" for key, value in row.items()])\n                    rows.append(row_text)\n                return \"\\n\".join(rows)\n        \n        else:\n            return f\"Format non supporte: {file_path.suffix}\"\n    \n    except Exception as e:\n        return f\"Erreur lecture fichier {file_path.name}: {str(e)}\"\n\ndef chunk_and_embed(text: str, source: str) -> List[dict]:\n    \"\"\"Decoupe le texte en chunks et genere les embeddings\"\"\"\n    # Chunking\n    chunks = text_splitter.split_text(text)\n    \n    # Embeddings\n    chunk_embeddings = embeddings.embed_documents(chunks)\n    \n    # Preparer les points pour Qdrant\n    points = []\n    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n        point = PointStruct(\n            id=str(uuid.uuid4()),\n            vector=embedding,\n            payload={\n                \"text\": chunk,\n                \"source\": source,\n                \"chunk_index\": i\n            }\n        )\n        points.append(point)\n    \n    return points\n\nprint(\"Fonctions utilitaires definies\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload_function",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Fonction d'upload de documents\n\ndef upload_documents(files):\n    \"\"\"Traite et stocke les documents uploades\"\"\"\n    if not files:\n        return \"[ERREUR] Aucun fichier uploade\"\n    \n    # S'assurer que la collection existe\n    create_collection_if_not_exists()\n    \n    total_chunks = 0\n    results = []\n    \n    for file in files:\n        try:\n            # Extraction du texte\n            file_path = file.name if hasattr(file, 'name') else file\n            filename = Path(file_path).name\n            text = extract_text_from_file(file_path)\n            \n            if text.startswith(\"Erreur\") or text.startswith(\"Format non supporte\"):\n                results.append(f\"[WARN] {filename}: {text}\")\n                continue\n            \n            # Chunking et embedding\n            points = chunk_and_embed(text, filename)\n            \n            # Stockage dans Qdrant\n            qdrant_client.upsert(\n                collection_name=COLLECTION_NAME,\n                points=points\n            )\n            \n            total_chunks += len(points)\n            results.append(f\"[OK] {filename}: {len(points)} chunks\")\n            \n        except Exception as e:\n            results.append(f\"[ERREUR] {filename}: Erreur - {str(e)}\")\n    \n    summary = f\"\\n\\n**Total: {total_chunks} chunks** stockes dans '{COLLECTION_NAME}'\"\n    return \"\\n\".join(results) + summary\n\nprint(\"Fonction upload_documents definie\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_function",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Fonction RAG\n\ndef search_and_answer(question: str, top_k: int = 3):\n    \"\"\"Recherche dans Qdrant et genere une reponse avec Mistral\"\"\"\n    if not question or question.strip() == \"\":\n        return \"[WARN] Veuillez poser une question\"\n    \n    try:\n        # 1. Generer l'embedding de la question\n        question_embedding = embeddings.embed_query(question)\n        \n        # 2. Recherche dans Qdrant (methode correcte: search)\n        search_results = qdrant_client.search(\n            collection_name=COLLECTION_NAME,\n            query_vector=question_embedding,\n            limit=top_k\n        )\n        \n        # 3. Extraire les chunks pertinents\n        if not search_results:\n            return \"[WARN] Aucun document trouve. Uploadez d'abord des documents.\"\n        \n        contexts = []\n        sources = []\n        for point in search_results:\n            contexts.append(point.payload[\"text\"])\n            source = point.payload.get(\"source\", \"Unknown\")\n            if source not in sources:\n                sources.append(source)\n        \n        context_text = \"\\n\\n---\\n\\n\".join(contexts)\n        \n        # 4. Construire le prompt\n        prompt = f\"\"\"Tu es un assistant qui repond aux questions en te basant UNIQUEMENT sur le contexte fourni.\n\nContexte:\n{context_text}\n\nQuestion: {question}\n\nInstructions:\n- Reponds de maniere claire et concise\n- Base-toi UNIQUEMENT sur le contexte fourni\n- Si l'information n'est pas dans le contexte, dis-le clairement\n- Cite les sources quand pertinent\n\nReponse:\"\"\"\n        \n        # 5. Generer la reponse avec Mistral\n        response = llm.invoke(prompt)\n        answer = response.content\n        \n        # 6. Formater la reponse\n        output = f\"\"\"## Reponse\n\n{answer}\n\n---\n\n**Sources consultees:** {', '.join(sources)}\n**{len(contexts)} chunks** analyses\n\"\"\"\n        \n        return output\n    \n    except Exception as e:\n        return f\"[ERREUR] {str(e)}\\n\\nDetails technique: {type(e).__name__}\"\n\nprint(\"Fonction search_and_answer definie\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio_interface",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Interface Gradio\n\nwith gr.Blocks(title=\"GreenPower RAG v04\", theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\n        \"\"\"\n        # GreenPower RAG System v04\n        \n        **Retrieval-Augmented Generation** pour vos documents GreenPower\n        \n        ### Workflow:\n        1. **Upload** vos documents (PDF, DOCX, TXT, JSON, CSV)\n        2. **Ask** vos questions sur le contenu\n        3. Obtenez des reponses basees sur vos documents!\n        \"\"\"\n    )\n    \n    with gr.Tab(\"Upload Documents\"):\n        gr.Markdown(\n            \"\"\"\n            ### Upload vos documents\n            \n            Formats supportes: **PDF**, **DOCX**, **TXT**, **JSON**, **CSV**\n            \n            Les documents seront:\n            - Decoupes en chunks\n            - Vectorises avec sentence-transformers\n            - Stockes dans Qdrant\n            \"\"\"\n        )\n        \n        file_input = gr.File(\n            file_count=\"multiple\",\n            label=\"Selectionnez vos documents\",\n            file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".json\", \".csv\"]\n        )\n        upload_btn = gr.Button(\"Upload et Traiter\", variant=\"primary\")\n        upload_output = gr.Textbox(\n            label=\"Resultat\",\n            lines=10,\n            placeholder=\"Les resultats d'upload apparaitront ici...\"\n        )\n        \n        upload_btn.click(\n            upload_documents,\n            inputs=file_input,\n            outputs=upload_output\n        )\n    \n    with gr.Tab(\"Ask Questions\"):\n        gr.Markdown(\n            \"\"\"\n            ### Posez vos questions\n            \n            Le systeme va:\n            1. Rechercher les passages pertinents\n            2. Generer une reponse avec Mistral\n            3. Citer les sources utilisees\n            \"\"\"\n        )\n        \n        question_input = gr.Textbox(\n            label=\"Votre question\",\n            placeholder=\"Ex: Quels sont les objectifs de GreenPower pour 2025?\",\n            lines=3\n        )\n        \n        top_k_slider = gr.Slider(\n            minimum=1,\n            maximum=10,\n            value=3,\n            step=1,\n            label=\"Nombre de chunks a recuperer\",\n            info=\"Plus de chunks = plus de contexte (mais plus lent)\"\n        )\n        \n        ask_btn = gr.Button(\"Obtenir la Reponse\", variant=\"primary\")\n        \n        answer_output = gr.Markdown(\n            label=\"Reponse\",\n            value=\"*La reponse apparaitra ici...*\"\n        )\n        \n        ask_btn.click(\n            search_and_answer,\n            inputs=[question_input, top_k_slider],\n            outputs=answer_output\n        )\n        \n        gr.Examples(\n            examples=[\n                [\"Quels sont les objectifs principaux de l'entreprise?\", 3],\n                [\"Quelle est la strategie de developpement?\", 5],\n                [\"Quels sont les projets en cours?\", 3],\n            ],\n            inputs=[question_input, top_k_slider],\n        )\n    \n    with gr.Tab(\"Info\"):\n        gr.Markdown(\n            f\"\"\"\n            ### Configuration Technique\n            \n            - **Vector DB:** Qdrant (in-memory)\n            - **Embeddings:** {EMBEDDING_MODEL}\n            - **LLM:** Mistral Small\n            - **Collection:** {COLLECTION_NAME}\n            - **Chunk size:** {CHUNK_SIZE} caracteres\n            - **Overlap:** {CHUNK_OVERLAP} caracteres\n            \n            ### Notes\n            \n            - Le mode in-memory ne persiste pas les donnees entre les redemarrages\n            - Pour la production, utilisez Qdrant en mode serveur\n            - Les embeddings sont generes localement (CPU)\n            \n            ### Corrections v04\n            \n            - Fix `search()` -> `query_points()` (API Qdrant correcte)\n            - Fix creation collection avant upsert\n            - Fix imports langchain\n            - Gestion erreurs amelioree\n            - Meilleur formatage des reponses\n            - Support ajoute pour JSON et CSV\n            \"\"\"\n        )\n\n# Lancer l'interface\nprint(\"\\n\" + \"=\"*60)\nprint(\"Lancement de l'interface Gradio...\")\nprint(\"=\"*60)\n\ndemo.launch(\n    server_name=\"127.0.0.1\",\n    server_port=7650,\n    share=False,\n    show_error=True\n)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}