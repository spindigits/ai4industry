{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9bf820",
   "metadata": {},
   "source": [
    "# GreenPower RAG System v07 - Hybrid Versioning\n",
    "\n",
    "## Pipeline complet:\n",
    "1. Setup Qdrant (local vectoriel)\n",
    "2. Chargement documents GreenPower via Gradio\n",
    "3. **DÃ©tection automatique : donnÃ©es stables vs temporelles**\n",
    "4. Chunking + Embeddings (sentence-transformers)\n",
    "5. Stockage Qdrant avec **ID intelligents** (stable = Ã©crasement, temporal = historique)\n",
    "6. RAG pipeline : query â†’ retrieval â†’ **scan content for private_** â†’ filter â†’ Mistral\n",
    "7. Interface Gradio interactive\n",
    "\n",
    "## NouveautÃ©s v07 - Versioning Hybride:\n",
    "- âœ¨ **DonnÃ©es STABLES** : ID = `{filename}_{chunk_index}` â†’ Ã©crase anciennes versions\n",
    "  - Ex: politiques, procÃ©dures, descriptions produits\n",
    "- âœ¨ **DonnÃ©es TEMPORELLES** : ID = `{filename}_{timestamp}_{chunk_index}` â†’ garde historique\n",
    "  - Ex: prix, salaires, stocks, budgets, KPIs\n",
    "- âœ¨ **DÃ©tection auto** : patterns `prix`, `tarif`, `salaire`, `stock`, `budget` dans filename/contenu\n",
    "- âœ¨ **MÃ©tadonnÃ©es enrichies** : timestamp, version, type de donnÃ©e\n",
    "- ğŸ”’ **Filtrage private_** : toujours actif sur le contenu\n",
    "\n",
    "## Comment Ã§a marche:\n",
    "```\n",
    "Upload \"politique_rh.pdf\" â†’ STABLE â†’ Ã©crase l'ancienne version\n",
    "Upload \"prix_2025.csv\" â†’ TEMPORAL â†’ garde historique avec timestamp\n",
    "Upload \"prix_2025.csv\" (2e fois) â†’ Nouvelle version coexiste avec l'ancienne\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Installation des dÃ©pendances\n",
    "!pip install -q langchain-mistralai langchain-community langchain-text-splitters\n",
    "!pip install -q qdrant-client gradio sentence-transformers\n",
    "!pip install -q pypdf python-docx python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agaragar\\Documents\\00 CLIENTS\\0000 VALLUP\\00 MENSAFLOW\\00 CNAM\\UseCase\\Jupyter\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports OK\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import gradio as gr\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from typing import List, Tuple\n",
    "import pypdf\n",
    "import docx\n",
    "import json\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = Path('.env')\n",
    "if env_path.exists():\n",
    "    with open(env_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#') and '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "QDRANT_URL = os.getenv('QDRANT_URL', ':memory:')  # Use :memory: for local or cloud URL\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY', None)\n",
    "\n",
    "# ğŸ”’ PATTERN POUR DÃ‰TECTER LES DONNÃ‰ES PRIVÃ‰ES\n",
    "PRIVATE_PATTERN = re.compile(r'private_\\w+', re.IGNORECASE)\n",
    "\n",
    "# ğŸ“… PATTERNS POUR DÃ‰TECTER LES DONNÃ‰ES TEMPORELLES\n",
    "TEMPORAL_KEYWORDS = [\n",
    "    'prix', 'price', 'tarif', 'tarification',\n",
    "    'salaire', 'salary', 'paie', 'remuneration',\n",
    "    'stock', 'inventory', 'inventaire',\n",
    "    'budget', 'kpi', 'metric', 'metriques',\n",
    "    'vente', 'sales', 'ca', 'chiffre',\n",
    "    'cours', 'cotation', 'taux', 'rate'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "init_clients",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agaragar\\AppData\\Local\\Temp\\ipykernel_16328\\3300190634.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clients initialisÃ©s\n",
      "   - Qdrant: Mode https://e7c2066d-6b1b-43d1-be10-9e84939013a4.europe-west3-0.gcp.cloud.qdrant.io\n",
      "   - Embeddings: sentence-transformers/all-MiniLM-L6-v2\n",
      "   - LLM: Mistral Small\n",
      "   - ğŸ”’ Pattern privÃ©: private_\\w+\n",
      "   - ğŸ“… Mots-clÃ©s temporels: 23 patterns\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Initialisation des clients\n",
    "\n",
    "# Configuration CHUNKS\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Initialize components\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "llm = ChatMistralAI(model='mistral-small-latest', mistral_api_key=MISTRAL_API_KEY, temperature=0.7)\n",
    "\n",
    "qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "COLLECTION_NAME = \"greenpower_docs\"\n",
    "\n",
    "# Text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "print(\"âœ… Clients initialisÃ©s\")\n",
    "print(f\"   - Qdrant: Mode {QDRANT_URL}\")\n",
    "print(f\"   - Embeddings: {EMBEDDING_MODEL}\")\n",
    "print(f\"   - LLM: Mistral Small\")\n",
    "print(f\"   - ğŸ”’ Pattern privÃ©: {PRIVATE_PATTERN.pattern}\")\n",
    "print(f\"   - ğŸ“… Mots-clÃ©s temporels: {len(TEMPORAL_KEYWORDS)} patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "create_collection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Collection 'greenpower_docs' existe dÃ©jÃ \n"
     ]
    }
   ],
   "source": [
    "# Cell 5: CrÃ©ation de la collection Qdrant\n",
    "\n",
    "# Obtenir la dimension des embeddings en gÃ©nÃ©rant un embedding de test\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "VECTOR_SIZE = len(test_embedding)\n",
    "\n",
    "def create_collection_if_not_exists():\n",
    "    \"\"\"CrÃ©e la collection Qdrant si elle n'existe pas dÃ©jÃ \"\"\"\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections()\n",
    "        collection_names = [c.name for c in collections.collections]\n",
    "        \n",
    "        if COLLECTION_NAME in collection_names:\n",
    "            print(f\"â„¹ï¸ Collection '{COLLECTION_NAME}' existe dÃ©jÃ \")\n",
    "            return\n",
    "        \n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(\n",
    "                size=VECTOR_SIZE,\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        print(f\"âœ… Collection '{COLLECTION_NAME}' crÃ©Ã©e (dimension: {VECTOR_SIZE})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur crÃ©ation collection: {e}\")\n",
    "        raise\n",
    "\n",
    "def reset_collection():\n",
    "    \"\"\"ğŸ—‘ï¸ Supprime et recrÃ©e la collection (nettoyage complet)\"\"\"\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "        print(f\"ğŸ—‘ï¸ Collection '{COLLECTION_NAME}' supprimÃ©e\")\n",
    "    except:\n",
    "        pass\n",
    "    create_collection_if_not_exists()\n",
    "\n",
    "# CrÃ©er la collection\n",
    "create_collection_if_not_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fonctions utilitaires dÃ©finies\n",
      "   - DÃ©tection private_ dans le contenu\n",
      "   - DÃ©tection donnÃ©es temporelles (prix, stocks, etc.)\n",
      "   - Versioning hybride : stable vs temporal\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Fonctions utilitaires\n",
    "\n",
    "def extract_text_from_file(file_path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF, DOCX, TXT, JSON ou CSV\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    try:\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            with open(file_path, 'rb') as f:\n",
    "                pdf_reader = pypdf.PdfReader(f)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        \n",
    "        elif file_path.suffix.lower() in ['.docx', '.doc']:\n",
    "            doc = docx.Document(file_path)\n",
    "            return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.json':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    text_parts = []\n",
    "                    for key, value in data.items():\n",
    "                        if isinstance(value, (dict, list)):\n",
    "                            text_parts.append(f\"{key}: {json.dumps(value, ensure_ascii=False, indent=2)}\")\n",
    "                        else:\n",
    "                            text_parts.append(f\"{key}: {value}\")\n",
    "                    return \"\\n\".join(text_parts)\n",
    "                elif isinstance(data, list):\n",
    "                    return \"\\n\\n\".join([json.dumps(item, ensure_ascii=False, indent=2) for item in data])\n",
    "                else:\n",
    "                    return str(data)\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.csv':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                csv_reader = csv.DictReader(f)\n",
    "                rows = []\n",
    "                for row in csv_reader:\n",
    "                    row_text = \", \".join([f\"{key}: {value}\" for key, value in row.items()])\n",
    "                    rows.append(row_text)\n",
    "                return \"\\n\".join(rows)\n",
    "        \n",
    "        else:\n",
    "            return f\"Format non supportÃ©: {file_path.suffix}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Erreur lecture fichier {file_path.name}: {str(e)}\"\n",
    "\n",
    "def contains_private_data(text: str) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"ğŸ”’ DÃ©tecte si le texte contient des valeurs privÃ©es (private_xxx)\"\"\"\n",
    "    matches = PRIVATE_PATTERN.findall(text)\n",
    "    return len(matches) > 0, matches\n",
    "\n",
    "def is_temporal_data(filename: str, text_content: str) -> bool:\n",
    "    \"\"\"ğŸ“… DÃ©tecte si les donnÃ©es sont temporelles (Ã©voluent dans le temps)\n",
    "    \n",
    "    Analyse le nom du fichier et le contenu pour dÃ©tecter des patterns\n",
    "    indiquant des donnÃ©es qui changent avec le temps (prix, stocks, etc.)\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    content_lower = text_content.lower()[:2000]  # Analyser les 2000 premiers caractÃ¨res\n",
    "    \n",
    "    # Chercher les mots-clÃ©s temporels\n",
    "    for keyword in TEMPORAL_KEYWORDS:\n",
    "        if keyword in filename_lower or keyword in content_lower:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def chunk_and_embed(text: str, source: str, is_temporal: bool = False) -> List[dict]:\n",
    "    \"\"\"ğŸ“¦ DÃ©coupe le texte en chunks et gÃ©nÃ¨re les embeddings avec versioning intelligent\n",
    "    \n",
    "    Args:\n",
    "        text: Le texte Ã  chunker\n",
    "        source: Nom du fichier source\n",
    "        is_temporal: Si True, ajoute timestamp (garde historique), sinon ID stable (Ã©crasement)\n",
    "    \"\"\"\n",
    "    # Chunking\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Embeddings\n",
    "    chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "    \n",
    "    # Timestamp pour donnÃ©es temporelles\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\") if is_temporal else None\n",
    "    \n",
    "    # PrÃ©parer les points pour Qdrant\n",
    "    points = []\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        # ğŸ¯ ID INTELLIGENT : stable ou temporal\n",
    "        if is_temporal:\n",
    "            # DonnÃ©es temporelles : ID unique avec timestamp (garde historique)\n",
    "            point_id = f\"{source}_{timestamp}_{i}\"\n",
    "        else:\n",
    "            # DonnÃ©es stables : ID fixe (Ã©crase l'ancienne version)\n",
    "            point_id = f\"{source}_{i}\"\n",
    "        \n",
    "        point = PointStruct(\n",
    "            id=point_id,  # Plus uuid.uuid4() !\n",
    "            vector=embedding,\n",
    "            payload={\n",
    "                \"text\": chunk,\n",
    "                \"source\": source,\n",
    "                \"chunk_index\": i,\n",
    "                \"is_temporal\": is_temporal,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"indexed_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    return points\n",
    "\n",
    "print(\"âœ… Fonctions utilitaires dÃ©finies\")\n",
    "print(\"   - DÃ©tection private_ dans le contenu\")\n",
    "print(\"   - DÃ©tection donnÃ©es temporelles (prix, stocks, etc.)\")\n",
    "print(\"   - Versioning hybride : stable vs temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "upload_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fonction upload_documents dÃ©finie (versioning hybride)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Fonction d'upload de documents avec versioning hybride\n",
    "\n",
    "def upload_documents(files):\n",
    "    \"\"\"ğŸ“¤ Traite et stocke les documents avec versioning intelligent\"\"\"\n",
    "    if not files:\n",
    "        return \"âŒ Aucun fichier uploadÃ©\"\n",
    "    \n",
    "    # S'assurer que la collection existe\n",
    "    create_collection_if_not_exists()\n",
    "    \n",
    "    total_chunks = 0\n",
    "    temporal_chunks = 0\n",
    "    stable_chunks = 0\n",
    "    chunks_with_private_content = 0\n",
    "    results = []\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            # Extraction du texte\n",
    "            file_path = file.name if hasattr(file, 'name') else file\n",
    "            filename = Path(file_path).name\n",
    "            \n",
    "            text = extract_text_from_file(file_path)\n",
    "            \n",
    "            if text.startswith(\"Erreur\") or text.startswith(\"Format non supportÃ©\"):\n",
    "                results.append(f\"âš ï¸ {filename}: {text}\")\n",
    "                continue\n",
    "            \n",
    "            # ğŸ“… DÃ‰TECTION AUTOMATIQUE : donnÃ©es temporelles ou stables ?\n",
    "            is_temporal = is_temporal_data(filename, text)\n",
    "            \n",
    "            # Chunking et embedding avec versioning intelligent\n",
    "            points = chunk_and_embed(text, filename, is_temporal=is_temporal)\n",
    "            \n",
    "            # ğŸ” Analyser combien de chunks contiennent des donnÃ©es privÃ©es\n",
    "            private_chunks_in_doc = 0\n",
    "            for point in points:\n",
    "                has_private, _ = contains_private_data(point.payload[\"text\"])\n",
    "                if has_private:\n",
    "                    private_chunks_in_doc += 1\n",
    "            \n",
    "            # Stockage dans Qdrant (Ã©crase ou ajoute selon le type)\n",
    "            qdrant_client.upsert(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                points=points\n",
    "            )\n",
    "            \n",
    "            total_chunks += len(points)\n",
    "            chunks_with_private_content += private_chunks_in_doc\n",
    "            \n",
    "            if is_temporal:\n",
    "                temporal_chunks += len(points)\n",
    "                version_info = \"ğŸ“… TEMPORAL (garde historique)\"\n",
    "            else:\n",
    "                stable_chunks += len(points)\n",
    "                version_info = \"ğŸ“Œ STABLE (Ã©crase ancienne version)\"\n",
    "            \n",
    "            result_line = f\"âœ… {filename}: {len(points)} chunks - {version_info}\"\n",
    "            \n",
    "            if private_chunks_in_doc > 0:\n",
    "                result_line += f\" âš ï¸ ({private_chunks_in_doc} avec private_)\"\n",
    "            \n",
    "            results.append(result_line)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append(f\"âŒ {filename}: Erreur - {str(e)}\")\n",
    "    \n",
    "    summary = f\"\\n\\nğŸ“Š **RÃ©sumÃ© d'indexation:**\\n\"\n",
    "    summary += f\"   - Total: {total_chunks} chunks stockÃ©s\\n\"\n",
    "    summary += f\"   - ğŸ“Œ DonnÃ©es stables: {stable_chunks} chunks (Ã©crasent anciennes versions)\\n\"\n",
    "    summary += f\"   - ğŸ“… DonnÃ©es temporelles: {temporal_chunks} chunks (historique conservÃ©)\\n\"\n",
    "    \n",
    "    if chunks_with_private_content > 0:\n",
    "        summary += f\"\\nâš ï¸ {chunks_with_private_content} chunks contiennent des valeurs private_* (seront filtrÃ©s)\"\n",
    "    \n",
    "    return \"\\n\".join(results) + summary\n",
    "\n",
    "print(\"âœ… Fonction upload_documents dÃ©finie (versioning hybride)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rag_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fonction search_and_answer dÃ©finie (filtrage private_ + versioning)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Fonction RAG avec filtrage des contenus privÃ©s\n",
    "\n",
    "def search_and_answer(question: str, top_k: int = 3, include_history: bool = False):\n",
    "    \"\"\"ğŸ” Recherche dans Qdrant et gÃ©nÃ¨re une rÃ©ponse (filtre les chunks avec private_)\n",
    "    \n",
    "    Args:\n",
    "        question: Question de l'utilisateur\n",
    "        top_k: Nombre de chunks Ã  retourner\n",
    "        include_history: Si True, inclut les versions historiques des donnÃ©es temporelles\n",
    "    \"\"\"\n",
    "    if not question or question.strip() == \"\":\n",
    "        return \"âš ï¸ Veuillez poser une question\"\n",
    "    \n",
    "    try:\n",
    "        # 1. GÃ©nÃ©rer l'embedding de la question\n",
    "        question_embedding = embeddings.embed_query(question)\n",
    "        \n",
    "        # 2. Recherche dans Qdrant avec un top_k plus large pour compenser le filtrage\n",
    "        search_results = qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=question_embedding,\n",
    "            limit=top_k * 5  # Chercher 5x plus pour avoir assez aprÃ¨s filtrage\n",
    "        )\n",
    "        \n",
    "        if not search_results:\n",
    "            return \"âš ï¸ Aucun document trouvÃ©. Uploadez d'abord des documents.\"\n",
    "        \n",
    "        # 3. ğŸ”’ FILTRAGE DES CHUNKS CONTENANT DES DONNÃ‰ES PRIVÃ‰ES\n",
    "        filtered_results = []\n",
    "        private_chunks = []\n",
    "        private_values_found = set()\n",
    "        temporal_info = {}\n",
    "        \n",
    "        for point in search_results:\n",
    "            chunk_text = point.payload[\"text\"]\n",
    "            \n",
    "            # VÃ©rifier si le chunk contient des valeurs private_\n",
    "            has_private, private_matches = contains_private_data(chunk_text)\n",
    "            \n",
    "            if has_private:\n",
    "                # ğŸš« Ce chunk contient des donnÃ©es privÃ©es, on le stocke mais ne l'utilise pas\n",
    "                private_chunks.append({\n",
    "                    'source': point.payload.get('source', 'Unknown'),\n",
    "                    'matches': private_matches\n",
    "                })\n",
    "                private_values_found.update(private_matches)\n",
    "                continue\n",
    "            \n",
    "            # âœ… Chunk clean, on l'ajoute\n",
    "            filtered_results.append(point)\n",
    "            \n",
    "            # Tracker les donnÃ©es temporelles\n",
    "            if point.payload.get('is_temporal'):\n",
    "                source = point.payload.get('source')\n",
    "                timestamp = point.payload.get('timestamp')\n",
    "                if source not in temporal_info:\n",
    "                    temporal_info[source] = []\n",
    "                temporal_info[source].append(timestamp)\n",
    "            \n",
    "            # Limiter au top_k demandÃ©\n",
    "            if len(filtered_results) >= top_k:\n",
    "                break\n",
    "        \n",
    "        # 4. VÃ©rifier s'il reste des rÃ©sultats aprÃ¨s filtrage\n",
    "        if not filtered_results:\n",
    "            # Tous les rÃ©sultats contenaient des donnÃ©es privÃ©es\n",
    "            return f\"\"\"## ğŸ”’ DonnÃ©e confidentielle\n",
    "\n",
    "DÃ©solÃ©, les informations pertinentes pour cette question contiennent des **donnÃ©es confidentielles** et ne peuvent pas Ãªtre affichÃ©es.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ”’ **{len(private_chunks)} chunks** ont Ã©tÃ© trouvÃ©s mais contenaient des valeurs privÃ©es :\n",
    "{', '.join(sorted(private_values_found))}\n",
    "\n",
    "ğŸ’¡ Ces donnÃ©es sont protÃ©gÃ©es et ne seront jamais affichÃ©es dans les rÃ©ponses.\n",
    "\"\"\"\n",
    "        \n",
    "        # 5. Extraire les chunks pertinents (uniquement ceux sans donnÃ©es privÃ©es)\n",
    "        contexts = []\n",
    "        sources = []\n",
    "        for point in filtered_results:\n",
    "            contexts.append(point.payload[\"text\"])\n",
    "            source = point.payload.get(\"source\", \"Unknown\")\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        \n",
    "        context_text = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "        \n",
    "        # 6. Construire le prompt\n",
    "        prompt = f\"\"\"Tu es un assistant qui rÃ©pond aux questions en te basant UNIQUEMENT sur le contexte fourni.\n",
    "\n",
    "Contexte:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- RÃ©ponds de maniÃ¨re claire et concise\n",
    "- Base-toi UNIQUEMENT sur le contexte fourni\n",
    "- Si l'information n'est pas dans le contexte, dis-le clairement\n",
    "- Cite les sources quand pertinent\n",
    "\n",
    "RÃ©ponse:\"\"\"\n",
    "        \n",
    "        # 7. GÃ©nÃ©rer la rÃ©ponse avec Mistral\n",
    "        response = llm.invoke(prompt)\n",
    "        answer = response.content\n",
    "        \n",
    "        # 8. Formater la rÃ©ponse avec info sur filtrage\n",
    "        output = f\"\"\"## ğŸ’¬ RÃ©ponse\n",
    "\n",
    "{answer}\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ“š **Sources consultÃ©es:** {', '.join(sources)}\n",
    "ğŸ” **{len(contexts)} chunks** analysÃ©s (sans donnÃ©es confidentielles)\n",
    "\"\"\"\n",
    "        \n",
    "        # Ajouter info sur donnÃ©es temporelles\n",
    "        if temporal_info:\n",
    "            output += \"\\nğŸ“… **DonnÃ©es temporelles dÃ©tectÃ©es:**\\n\"\n",
    "            for source, timestamps in temporal_info.items():\n",
    "                latest = max(timestamps) if timestamps else \"N/A\"\n",
    "                output += f\"   - {source}: version du {latest}\\n\"\n",
    "        \n",
    "        # Ajouter un warning si des chunks privÃ©s ont Ã©tÃ© exclus\n",
    "        if private_chunks:\n",
    "            output += f\"\\nğŸ”’ **{len(private_chunks)} chunks** ont Ã©tÃ© exclus car ils contenaient des valeurs privÃ©es :\"\n",
    "            output += f\"\\n   {', '.join(sorted(private_values_found))}\"\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"âŒ Erreur: {str(e)}\\n\\nDÃ©tails technique: {type(e).__name__}\"\n",
    "\n",
    "print(\"âœ… Fonction search_and_answer dÃ©finie (filtrage private_ + versioning)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gradio_interface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ Lancement de l'interface Gradio v07 - Hybrid Versioning...\n",
      "======================================================================\n",
      "* Running on local URL:  http://127.0.0.1:7855\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7855/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9: Interface Gradio\n",
    "\n",
    "with gr.Blocks(title=\"GreenPower RAG v07 - Hybrid Versioning\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ğŸŒ± GreenPower RAG System v07 - ğŸ“… Hybrid Versioning + ğŸ”’ Private Filter\n",
    "        \n",
    "        **Retrieval-Augmented Generation** avec **versioning intelligent** et **filtrage des donnÃ©es privÃ©es**\n",
    "        \n",
    "        ### ğŸ“‹ Workflow:\n",
    "        1. **Upload** vos documents (PDF, DOCX, TXT, JSON, CSV)\n",
    "        2. **DÃ©tection automatique** : donnÃ©es stables vs temporelles\n",
    "        3. **Versioning intelligent** :\n",
    "           - ğŸ“Œ DonnÃ©es stables â†’ Ã©crasent l'ancienne version\n",
    "           - ğŸ“… DonnÃ©es temporelles â†’ garde l'historique avec timestamp\n",
    "        4. **Ask** vos questions\n",
    "        5. **Filtrage automatique** des chunks contenant `private_xxx`\n",
    "        \n",
    "        ### ğŸ¯ Types de donnÃ©es:\n",
    "        \n",
    "        **ğŸ“Œ STABLES** (Ã©crasement) :\n",
    "        - Politiques, procÃ©dures, rÃ¨glements\n",
    "        - Descriptions produits, services\n",
    "        - Documentation technique\n",
    "        - ID format: `{filename}_{chunk_index}`\n",
    "        \n",
    "        **ğŸ“… TEMPORELLES** (historique) :\n",
    "        - Prix, tarifs, cotations\n",
    "        - Salaires, budgets, KPIs\n",
    "        - Stocks, inventaires\n",
    "        - Ventes, chiffre d'affaires\n",
    "        - ID format: `{filename}_{timestamp}_{chunk_index}`\n",
    "        \n",
    "        ### ğŸ’¡ Exemple:\n",
    "        ```\n",
    "        Upload 1: \"politique_rh.pdf\" â†’ ğŸ“Œ STABLE\n",
    "        Upload 2: \"prix_2025_v1.csv\" â†’ ğŸ“… TEMPORAL (version 2025-01-20_143022)\n",
    "        Upload 3: \"prix_2025_v2.csv\" â†’ ğŸ“… TEMPORAL (version 2025-01-20_150533)\n",
    "        Upload 4: \"politique_rh.pdf\" (modifiÃ©) â†’ ğŸ“Œ STABLE (Ã©crase la v1)\n",
    "        \n",
    "        RÃ©sultat: 2 versions de prix coexistent, 1 seule version de politique_rh\n",
    "        ```\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Tab(\"ğŸ“¤ Upload Documents\"):\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### Upload vos documents\n",
    "            \n",
    "            Formats supportÃ©s: **PDF**, **DOCX**, **TXT**, **JSON**, **CSV**\n",
    "            \n",
    "            ğŸ¤– **DÃ©tection automatique du type de donnÃ©es:**\n",
    "            \n",
    "            Le systÃ¨me analyse le **nom du fichier** et le **contenu** pour dÃ©tecter :\n",
    "            \n",
    "            **ğŸ“… DonnÃ©es temporelles** (mots-clÃ©s dÃ©tectÃ©s) :\n",
    "            - `prix`, `price`, `tarif`, `tarification`\n",
    "            - `salaire`, `salary`, `paie`, `remuneration`\n",
    "            - `stock`, `inventory`, `inventaire`\n",
    "            - `budget`, `kpi`, `metric`, `metriques`\n",
    "            - `vente`, `sales`, `ca`, `chiffre`\n",
    "            - `cours`, `cotation`, `taux`, `rate`\n",
    "            \n",
    "            Si dÃ©tectÃ© â†’ **garde l'historique avec timestamp**\n",
    "            Sinon â†’ **Ã©crase l'ancienne version**\n",
    "            \n",
    "            ğŸ”’ **DonnÃ©es privÃ©es dans le contenu:**\n",
    "            - Utilisez le pattern `private_xxx` pour marquer des valeurs sensibles\n",
    "            - Ex: `\"Le client private_client_001 a achetÃ©...\"`\n",
    "            - Ces chunks seront automatiquement filtrÃ©s lors des recherches\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        file_input = gr.File(\n",
    "            file_count=\"multiple\",\n",
    "            label=\"ğŸ“ SÃ©lectionnez vos documents\",\n",
    "            file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".json\", \".csv\"]\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            upload_btn = gr.Button(\"ğŸš€ Upload et Traiter\", variant=\"primary\")\n",
    "            reset_btn = gr.Button(\"ğŸ—‘ï¸ Reset Collection\", variant=\"stop\")\n",
    "        \n",
    "        upload_output = gr.Textbox(\n",
    "            label=\"ğŸ“Š RÃ©sultat\",\n",
    "            lines=12,\n",
    "            placeholder=\"Les rÃ©sultats d'upload apparaÃ®tront ici...\"\n",
    "        )\n",
    "        \n",
    "        upload_btn.click(\n",
    "            upload_documents,\n",
    "            inputs=file_input,\n",
    "            outputs=upload_output\n",
    "        )\n",
    "        \n",
    "        def reset_and_notify():\n",
    "            reset_collection()\n",
    "            return \"âœ… Collection rÃ©initialisÃ©e ! Tous les documents ont Ã©tÃ© supprimÃ©s.\"\n",
    "        \n",
    "        reset_btn.click(\n",
    "            reset_and_notify,\n",
    "            inputs=None,\n",
    "            outputs=upload_output\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"ğŸ’¬ Ask Questions\"):\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### Posez vos questions\n",
    "            \n",
    "            Le systÃ¨me va:\n",
    "            1. ğŸ” Rechercher les passages pertinents dans Qdrant\n",
    "            2. ğŸ“… Prioriser les **versions les plus rÃ©centes** pour les donnÃ©es temporelles\n",
    "            3. ğŸ”’ **Scanner chaque chunk** pour dÃ©tecter des valeurs `private_*`\n",
    "            4. ğŸš« **Filtrer automatiquement** les chunks contenant des donnÃ©es privÃ©es\n",
    "            5. ğŸ§  GÃ©nÃ©rer une rÃ©ponse avec Mistral (uniquement donnÃ©es publiques)\n",
    "            6. ğŸ“š Citer les sources et versions utilisÃ©es\n",
    "            \n",
    "            âš ï¸ Si tous les chunks pertinents contiennent `private_*` â†’ **\"DÃ©solÃ©, donnÃ©e confidentielle\"**\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        question_input = gr.Textbox(\n",
    "            label=\"â“ Votre question\",\n",
    "            placeholder=\"Ex: Quels sont les prix actuels? Quelle est notre politique RH?\",\n",
    "            lines=3\n",
    "        )\n",
    "        \n",
    "        top_k_slider = gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=10,\n",
    "            value=3,\n",
    "            step=1,\n",
    "            label=\"ğŸ¯ Nombre de chunks Ã  rÃ©cupÃ©rer\",\n",
    "            info=\"Plus de chunks = plus de contexte (mais plus lent)\"\n",
    "        )\n",
    "        \n",
    "        ask_btn = gr.Button(\"ğŸ¤” Obtenir la RÃ©ponse\", variant=\"primary\")\n",
    "        \n",
    "        answer_output = gr.Markdown(\n",
    "            label=\"ğŸ’¡ RÃ©ponse\",\n",
    "            value=\"*La rÃ©ponse apparaÃ®tra ici...*\"\n",
    "        )\n",
    "        \n",
    "        ask_btn.click(\n",
    "            search_and_answer,\n",
    "            inputs=[question_input, top_k_slider],\n",
    "            outputs=answer_output\n",
    "        )\n",
    "        \n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"Quels sont les prix actuels?\", 3],\n",
    "                [\"Quelle est notre politique de congÃ©s?\", 3],\n",
    "                [\"Montrez-moi l'Ã©volution des stocks\", 5],\n",
    "                [\"Quels sont les objectifs 2025?\", 3],\n",
    "            ],\n",
    "            inputs=[question_input, top_k_slider],\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"â„¹ï¸ Info\"):\n",
    "        gr.Markdown(\n",
    "            f\"\"\"\n",
    "            ### ğŸ”§ Configuration Technique\n",
    "            \n",
    "            - **Vector DB:** Qdrant ({QDRANT_URL})\n",
    "            - **Embeddings:** {EMBEDDING_MODEL}\n",
    "            - **LLM:** Mistral Small\n",
    "            - **Collection:** {COLLECTION_NAME}\n",
    "            - **Chunk size:** {CHUNK_SIZE} caractÃ¨res\n",
    "            - **Overlap:** {CHUNK_OVERLAP} caractÃ¨res\n",
    "            - **ğŸ”’ Pattern privÃ©:** `{PRIVATE_PATTERN.pattern}`\n",
    "            - **ğŸ“… Mots-clÃ©s temporels:** {len(TEMPORAL_KEYWORDS)} patterns\n",
    "            \n",
    "            ### ğŸ“… SystÃ¨me de Versioning Hybride\n",
    "            \n",
    "            **ProblÃ¨me rÃ©solu:**\n",
    "            - Certaines donnÃ©es changent souvent (prix, stocks) â†’ besoin d'historique\n",
    "            - D'autres sont stables (politiques, procÃ©dures) â†’ pas besoin d'historique\n",
    "            - Re-upload d'un fichier : faut-il Ã©craser ou garder les 2 versions ?\n",
    "            \n",
    "            **Solution v07:**\n",
    "            \n",
    "            1ï¸âƒ£ **DÃ©tection automatique du type de donnÃ©es**\n",
    "            ```python\n",
    "            # Analyse filename + contenu\n",
    "            is_temporal = any(keyword in text for keyword in [\n",
    "                'prix', 'salaire', 'stock', 'budget', 'kpi', 'vente'...\n",
    "            ])\n",
    "            ```\n",
    "            \n",
    "            2ï¸âƒ£ **ID intelligent basÃ© sur le type**\n",
    "            ```python\n",
    "            # DonnÃ©es STABLES (Ã©crasement)\n",
    "            ID = \"politique_rh.pdf_5\"  # Toujours le mÃªme ID\n",
    "            â†’ Re-upload Ã©crase l'ancien chunk\n",
    "            \n",
    "            # DonnÃ©es TEMPORELLES (historique)\n",
    "            ID = \"prix_2025.csv_2025-01-20_143022_5\"  # ID unique avec timestamp\n",
    "            â†’ Re-upload crÃ©e un nouveau chunk, garde l'ancien\n",
    "            ```\n",
    "            \n",
    "            3ï¸âƒ£ **MÃ©tadonnÃ©es enrichies**\n",
    "            ```json\n",
    "            {{\n",
    "                \"text\": \"...\",\n",
    "                \"source\": \"prix_2025.csv\",\n",
    "                \"chunk_index\": 5,\n",
    "                \"is_temporal\": true,\n",
    "                \"timestamp\": \"2025-01-20_143022\",\n",
    "                \"indexed_at\": \"2025-01-20T14:30:22\"\n",
    "            }}\n",
    "            ```\n",
    "            \n",
    "            **Cas d'usage concrets:**\n",
    "            \n",
    "            ğŸ“Œ **DonnÃ©es STABLES (Ã©crasement)**\n",
    "            ```\n",
    "            Upload 1: politique_rh.pdf (v1, 2024)\n",
    "            â†’ StockÃ© avec ID: politique_rh.pdf_0, politique_rh.pdf_1...\n",
    "            \n",
    "            Upload 2: politique_rh.pdf (v2, 2025, politique mise Ã  jour)\n",
    "            â†’ Ã‰CRASE les chunks avec mÃªmes IDs\n",
    "            â†’ Seule la v2 reste en base\n",
    "            \n",
    "            Avantage: Pas de pollution, toujours la derniÃ¨re version\n",
    "            ```\n",
    "            \n",
    "            ğŸ“… **DonnÃ©es TEMPORELLES (historique)**\n",
    "            ```\n",
    "            Upload 1: prix_produits.csv (janvier 2025)\n",
    "            â†’ StockÃ©: prix_produits.csv_2025-01-20_100000_0...\n",
    "            \n",
    "            Upload 2: prix_produits.csv (fÃ©vrier 2025, prix augmentÃ©s)\n",
    "            â†’ StockÃ©: prix_produits.csv_2025-02-15_140000_0...\n",
    "            â†’ Les 2 versions COEXISTENT\n",
    "            \n",
    "            Question: \"Quels sont les prix actuels?\"\n",
    "            â†’ Utilise la version fÃ©vrier (plus rÃ©cente)\n",
    "            \n",
    "            Question: \"Quelle Ã©tait l'Ã©volution des prix en janvier?\"\n",
    "            â†’ Peut comparer janvier vs fÃ©vrier\n",
    "            \n",
    "            Avantage: Analyse d'Ã©volution, traÃ§abilitÃ©, audit\n",
    "            ```\n",
    "            \n",
    "            ### ğŸ”’ SystÃ¨me de ConfidentialitÃ© (inchangÃ© depuis v05)\n",
    "            \n",
    "            **Filtrage par contenu:**\n",
    "            - Regex: `/private_\\\\w+/` (case insensitive)\n",
    "            - AppliquÃ© APRÃˆS la recherche vectorielle\n",
    "            - Chunks avec `private_xxx` â†’ automatiquement rejetÃ©s\n",
    "            - Message transparent si donnÃ©es filtrÃ©es\n",
    "            \n",
    "            **Patterns dÃ©tectÃ©s:**\n",
    "            - `private_client_001` âœ…\n",
    "            - `Private_Salary_Data` âœ…\n",
    "            - `PRIVATE_PROJECT_X` âœ…\n",
    "            \n",
    "            ### ğŸ“ Notes Importantes\n",
    "            \n",
    "            - ğŸ“Š Les chunks temporels incluent la version/timestamp dans la rÃ©ponse\n",
    "            - ğŸ¯ Recherche vectorielle priorise naturellement les versions rÃ©centes (plus de contexte)\n",
    "            - ğŸ—‘ï¸ Utilisez \"Reset Collection\" pour nettoyer complÃ¨tement la base\n",
    "            - ğŸ’¾ Mode `:memory:` ne persiste pas entre redÃ©marrages\n",
    "            - ğŸ”„ Pour production: utiliser Qdrant cloud/serveur avec persistence\n",
    "            \n",
    "            ### âœ¨ NouveautÃ©s v07\n",
    "            \n",
    "            - âœ… DÃ©tection automatique donnÃ©es temporelles vs stables\n",
    "            - âœ… ID intelligents : stables (Ã©crasement) vs temporels (historique)\n",
    "            - âœ… Timestamp automatique pour donnÃ©es temporelles\n",
    "            - âœ… MÃ©tadonnÃ©es enrichies (is_temporal, timestamp, indexed_at)\n",
    "            - âœ… Info versions dans les rÃ©ponses\n",
    "            - âœ… Bouton Reset Collection pour nettoyage\n",
    "            - âœ… CompatibilitÃ© avec filtrage private_ (v05)\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "# Lancer l'interface\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ Lancement de l'interface Gradio v07 - Hybrid Versioning...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "demo.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=7855,\n",
    "    share=False,\n",
    "    show_error=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
