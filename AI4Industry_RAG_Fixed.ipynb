{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI4Industry - Mensaflow ¬© - RAG avec Ollama + Qdrant\n",
        "## Architecture: Mistral + Vector DB + Agents (Math + Wikipedia)\n",
        "\n",
        "**Corrig√© par Claude - Janvier 2025**"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Installation des d√©pendances"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installation Ollama\n",
        "!sudo apt-get install -y zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Installation des packages Python\n",
        "!pip install -q langchain==0.3.20 langchain-community==0.3.20 langchain-core==0.3.40\n",
        "!pip install -q qdrant-client sentence-transformers wikipedia\n",
        "!pip install -q numexpr sympy"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ D√©marrage du serveur Ollama"
      ],
      "metadata": {
        "id": "ollama_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# D√©marrer Ollama en arri√®re-plan\n",
        "print(\"üîÑ D√©marrage du serveur Ollama...\")\n",
        "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(5)\n",
        "\n",
        "# T√©l√©charger Mistral 7B\n",
        "print(\"üì• T√©l√©chargement de Mistral 7B...\")\n",
        "!ollama pull mistral\n",
        "print(\"‚úÖ Mistral pr√™t !\")"
      ],
      "metadata": {
        "id": "start_ollama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Initialisation du LLM"
      ],
      "metadata": {
        "id": "llm_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "llm = Ollama(\n",
        "    model=\"mistral\",\n",
        "    base_url=\"http://localhost:11434\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Test rapide\n",
        "response = llm.invoke(\"Bonjour, qui es-tu ?\")\n",
        "print(\"ü§ñ Mistral:\", response[:200])"
      ],
      "metadata": {
        "id": "init_llm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üóÉÔ∏è Configuration de Qdrant (Vector Database)"
      ],
      "metadata": {
        "id": "qdrant_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "\n",
        "# Embedding model\n",
        "print(\"üìä Chargement du mod√®le d'embeddings...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "# Client Qdrant (en m√©moire pour Colab)\n",
        "print(\"üóÑÔ∏è Initialisation de Qdrant...\")\n",
        "qdrant_client = QdrantClient(\":memory:\")\n",
        "\n",
        "collection_name = \"ai4industry_docs\"\n",
        "\n",
        "# Cr√©er la collection\n",
        "qdrant_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Qdrant pr√™t !\")"
      ],
      "metadata": {
        "id": "init_qdrant"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Alimentation de la base vectorielle (Wikipedia)"
      ],
      "metadata": {
        "id": "feed_db_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Configurer Wikipedia en fran√ßais\n",
        "wikipedia.set_lang(\"fr\")\n",
        "\n",
        "# Topics pertinents pour l'industrie\n",
        "topics = [\n",
        "    \"Intelligence artificielle\",\n",
        "    \"Machine learning\",\n",
        "    \"Industrie 4.0\",\n",
        "    \"Maintenance pr√©dictive\",\n",
        "    \"Internet des objets\"\n",
        "]\n",
        "\n",
        "print(\"üìñ R√©cup√©ration des articles Wikipedia...\")\n",
        "documents = []\n",
        "\n",
        "for topic in topics:\n",
        "    try:\n",
        "        page = wikipedia.page(topic)\n",
        "        doc = Document(\n",
        "            page_content=page.content,\n",
        "            metadata={\"source\": topic, \"url\": page.url}\n",
        "        )\n",
        "        documents.append(doc)\n",
        "        print(f\"  ‚úì {topic}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó {topic}: {e}\")\n",
        "\n",
        "# D√©coupage en chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"\\nüìù {len(chunks)} chunks cr√©√©s\")\n",
        "\n",
        "# Indexation dans Qdrant\n",
        "print(\"üîÑ Indexation dans Qdrant...\")\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    chunks,\n",
        "    embeddings,\n",
        "    client=qdrant_client,\n",
        "    collection_name=collection_name\n",
        ")\n",
        "print(\"‚úÖ Base vectorielle pr√™te !\")"
      ],
      "metadata": {
        "id": "feed_vectordb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Cr√©ation des outils (Tools)"
      ],
      "metadata": {
        "id": "tools_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import Tool\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "import numexpr\n",
        "\n",
        "# 1. Outil de calcul math√©matique\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"√âvalue une expression math√©matique.\"\"\"\n",
        "    try:\n",
        "        result = numexpr.evaluate(expression).item()\n",
        "        return f\"R√©sultat: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Erreur de calcul: {str(e)}\"\n",
        "\n",
        "math_tool = Tool(\n",
        "    name=\"Calculator\",\n",
        "    func=calculate,\n",
        "    description=\"Utile pour les calculs math√©matiques. Exemple: '2+2' ou '10*5'\"\n",
        ")\n",
        "\n",
        "# 2. Outil Wikipedia\n",
        "wikipedia_wrapper = WikipediaAPIWrapper(lang=\"fr\")\n",
        "wikipedia_tool = Tool(\n",
        "    name=\"Wikipedia\",\n",
        "    func=wikipedia_wrapper.run,\n",
        "    description=\"Recherche des informations sur Wikipedia en fran√ßais\"\n",
        ")\n",
        "\n",
        "# 3. Outil RAG (retrieval)\n",
        "def rag_search(query: str) -> str:\n",
        "    \"\"\"Recherche dans la base vectorielle.\"\"\"\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    return context\n",
        "\n",
        "rag_tool = Tool(\n",
        "    name=\"RAG_Search\",\n",
        "    func=rag_search,\n",
        "    description=\"Recherche dans la base de connaissances vectorielle (documents index√©s)\"\n",
        ")\n",
        "\n",
        "tools = [math_tool, wikipedia_tool, rag_tool]\n",
        "print(\"‚úÖ 3 outils cr√©√©s: Calculator, Wikipedia, RAG_Search\")"
      ],
      "metadata": {
        "id": "create_tools"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Cr√©ation de l'Agent (CORRIG√â)"
      ],
      "metadata": {
        "id": "agent_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CORRECTION: Import depuis langchain.agents (nouvelle API)\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Template pour l'agent ReAct\n",
        "template = \"\"\"Tu es un assistant IA sp√©cialis√© dans l'industrie et la technologie.\n",
        "\n",
        "Tu as acc√®s aux outils suivants:\n",
        "{tools}\n",
        "\n",
        "Noms des outils: {tool_names}\n",
        "\n",
        "Utilise ce format:\n",
        "Question: la question pos√©e\n",
        "Thought: je dois r√©fl√©chir √† quel outil utiliser\n",
        "Action: le nom de l'outil (un parmi [{tool_names}])\n",
        "Action Input: l'entr√©e pour l'outil\n",
        "Observation: le r√©sultat de l'outil\n",
        "... (r√©p√®te Thought/Action/Action Input/Observation si n√©cessaire)\n",
        "Thought: je connais maintenant la r√©ponse finale\n",
        "Final Answer: la r√©ponse finale √† la question\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Cr√©er l'agent\n",
        "agent = create_react_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    prompt=prompt\n",
        ")\n",
        "\n",
        "# Cr√©er l'executor\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    max_iterations=5\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Agent RAG cr√©√© avec succ√®s !\")"
      ],
      "metadata": {
        "id": "create_agent"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Tests de l'Agent"
      ],
      "metadata": {
        "id": "test_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Calcul math√©matique\n",
        "print(\"=\"*60)\n",
        "print(\"TEST 1: Calcul math√©matique\")\n",
        "print(\"=\"*60)\n",
        "response = agent_executor.invoke({\"input\": \"Calcule 147 * 23 + 456\"})\n",
        "print(\"\\nüìä R√©ponse:\", response['output'])"
      ],
      "metadata": {
        "id": "test_math"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2: Recherche Wikipedia\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST 2: Recherche Wikipedia\")\n",
        "print(\"=\"*60)\n",
        "response = agent_executor.invoke({\"input\": \"Qu'est-ce que l'Industrie 4.0 ?\"})\n",
        "print(\"\\nüìö R√©ponse:\", response['output'])"
      ],
      "metadata": {
        "id": "test_wiki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 3: RAG (recherche vectorielle)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST 3: RAG Search\")\n",
        "print(\"=\"*60)\n",
        "response = agent_executor.invoke({\"input\": \"Parle-moi du machine learning dans l'industrie\"})\n",
        "print(\"\\nüîç R√©ponse:\", response['output'])"
      ],
      "metadata": {
        "id": "test_rag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 4: Question complexe combinant plusieurs outils\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST 4: Question complexe\")\n",
        "print(\"=\"*60)\n",
        "response = agent_executor.invoke({\n",
        "    \"input\": \"Si une usine utilise 3 capteurs IoT qui g√©n√®rent chacun 500 MB de donn√©es par jour, combien de GB cela repr√©sente en une semaine ?\"\n",
        "})\n",
        "print(\"\\nüè≠ R√©ponse:\", response['output'])"
      ],
      "metadata": {
        "id": "test_complex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí¨ Interface Interactive"
      ],
      "metadata": {
        "id": "interactive_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    print(\"\\nü§ñ Agent RAG AI4Industry - Mensaflow\")\n",
        "    print(\"Type 'exit' pour quitter\\n\")\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(\"Vous: \")\n",
        "        if user_input.lower() in ['exit', 'quit', 'sortir']:\n",
        "            print(\"Au revoir !\")\n",
        "            break\n",
        "        \n",
        "        try:\n",
        "            response = agent_executor.invoke({\"input\": user_input})\n",
        "            print(f\"\\nAgent: {response['output']}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur: {str(e)}\\n\")\n",
        "\n",
        "# Lancer le chat\n",
        "chat()"
      ],
      "metadata": {
        "id": "interactive_chat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Statistiques et Diagnostics"
      ],
      "metadata": {
        "id": "stats_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# V√©rifier le nombre de documents dans Qdrant\n",
        "collection_info = qdrant_client.get_collection(collection_name)\n",
        "print(f\"üìà Nombre de vectors dans Qdrant: {collection_info.vectors_count}\")\n",
        "\n",
        "# Test de similarit√©\n",
        "query = \"maintenance pr√©dictive\"\n",
        "results = vectorstore.similarity_search(query, k=3)\n",
        "print(f\"\\nüîç Top 3 r√©sultats pour '{query}':\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"   Extrait: {doc.page_content[:150]}...\")"
      ],
      "metadata": {
        "id": "diagnostics"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üéØ R√©sum√© de l'Architecture\n",
        "\n",
        "**Stack Technique:**\n",
        "- **LLM**: Mistral 7B (via Ollama)\n",
        "- **Vector DB**: Qdrant (in-memory)\n",
        "- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2\n",
        "- **Agents**: Calculator, Wikipedia, RAG Search\n",
        "- **Framework**: LangChain 0.3.x\n",
        "\n",
        "**Corrections appliqu√©es:**\n",
        "1. ‚úÖ Import correct de `AgentExecutor` (depuis `langchain.agents`)\n",
        "2. ‚úÖ Utilisation de `create_react_agent` moderne\n",
        "3. ‚úÖ Template ReAct compatible\n",
        "4. ‚úÖ Gestion d'erreurs am√©lior√©e\n",
        "\n",
        "**Mensaflow ¬© 2025 - Formation AI4Industry CNAM**"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
