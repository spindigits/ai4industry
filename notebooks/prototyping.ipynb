{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hybrid RAG System Development\n",
                "\n",
                "This notebook covers the step-by-step development of the RAG system."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv('../.env')\n",
                "\n",
                "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
                "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
                "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
                "\n",
                "assert GROQ_API_KEY, \"GROQ_API_KEY not found\"\n",
                "print(\"Environment keys loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Vector Database Setup (Qdrant)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from qdrant_client import QdrantClient\n",
                "from qdrant_client.http.models import Distance, VectorParams\n",
                "\n",
                "if QDRANT_URL and QDRANT_API_KEY:\n",
                "    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
                "else:\n",
                "    # Local memory mode for testing\n",
                "    client = QdrantClient(location=\":memory:\")\n",
                "\n",
                "collection_name = \"dev_rag_collection\"\n",
                "\n",
                "if not client.collection_exists(collection_name):\n",
                "    client.create_collection(\n",
                "        collection_name=collection_name,\n",
                "        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
                "    )\n",
                "    print(f\"Collection '{collection_name}' created.\")\n",
                "else:\n",
                "    print(f\"Collection '{collection_name}' exists.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Ingestion Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_groq import ChatGroq\n",
                "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
                "from langchain_qdrant import QdrantVectorStore\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_core.documents import Document\n",
                "\n",
                "# Setup Embeddings\n",
                "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "# Vector Store wrapper\n",
                "vector_store = QdrantVectorStore(\n",
                "    client=client,\n",
                "    collection_name=collection_name,\n",
                "    embedding=embeddings,\n",
                ")\n",
                "\n",
                "# Example Document\n",
                "text = \"\"\"\n",
                "LangChain is a framework for developing applications powered by language models. \n",
                "Qdrant is a vector similarity search engine and vector database.\n",
                "Streamlit turns data scripts into shareable web apps in minutes.\n",
                "\"\"\"\n",
                "docs = [Document(page_content=text, metadata={\"source\": \"dummy\"})]\n",
                "\n",
                "# Splitting\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
                "splits = text_splitter.split_documents(docs)\n",
                "\n",
                "# Add to Vector Store\n",
                "vector_store.add_documents(splits)\n",
                "print(f\"Added {len(splits)} chunks to Qdrant.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Retrieval and Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chains import create_retrieval_chain\n",
                "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "\n",
                "llm = ChatGroq(\n",
                "    groq_api_key=GROQ_API_KEY,\n",
                "    model_name=\"llama3-70b-8192\"\n",
                ")\n",
                "\n",
                "retriever = vector_store.as_retriever()\n",
                "\n",
                "system_prompt = (\n",
                "    \"You are an assistant for question-answering tasks. \"\n",
                "    \"Use the following pieces of retrieved context to answer \"\n",
                "    \"the question. If you don't know the answer, say that you \"\n",
                "    \"don't know.\"\n",
                "    \"\\n\\n\"\n",
                "    \"{context}\"\n",
                ")\n",
                "\n",
                "prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", system_prompt),\n",
                "        (\"human\", \"{input}\"),\n",
                "    ]\n",
                ")\n",
                "\n",
                "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
                "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
                "\n",
                "response = rag_chain.invoke({\"input\": \"What is Qdrant?\"})\n",
                "print(\"Answer:\", response[\"answer\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. (Extension) Graph Knowledge Experiment\n",
                "Here you can experiment with NetworkX or Neo4j integration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}